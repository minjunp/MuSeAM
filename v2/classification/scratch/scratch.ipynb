{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# IMPORT LIBRARIES\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from data_preprocess import preprocess\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/franciscogrisanti/anaconda3/envs/museam_env/bin/python'"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocess import preprocess\n",
    "\n",
    "fasta_file_positive = 'top_10percent.fa'\n",
    "fasta_file_negative = 'bottom_10percent.fa'\n",
    "\n",
    "# Preprocess the data\n",
    "positive_control = preprocess(f'./data/{fasta_file_positive}','./wt_readout.dat')\n",
    "positive_control = positive_control.one_hot_encode()\n",
    "positive_control_names = prep.read_fasta_name_into_array()\n",
    "\n",
    "negative_control = preprocess(f'./data/{fasta_file_negative}','./wt_readout.dat')\n",
    "negative_control  = prep.one_hot_encode()\n",
    "negative_control_names = prep.read_fasta_name_into_array()\n",
    "\n",
    "features = np.append(positive_control['forward'],negative_control['forward'],axis=0)\n",
    "targets = np.append(np.ones(len(positive_control['forward'])), np.zeros(len(negative_control['forward'])))      \n",
    "#names = np.append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET FUNCTIONS\n",
    "\n",
    "# Get dictionary from text file\n",
    "def train(file_name):\n",
    "    dict = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           dict[key] = val\n",
    "\n",
    "    # change string values to integer values\n",
    "    dict[\"filters\"] = int(dict[\"filters\"])\n",
    "    dict[\"kernel_size\"] = int(dict[\"kernel_size\"])\n",
    "    dict[\"epochs\"] = int(dict[\"epochs\"])\n",
    "    dict[\"batch_size\"] = int(dict[\"batch_size\"])\n",
    "    dict[\"validation_split\"] = float(dict[\"validation_split\"])    \n",
    "    return dict\n",
    "\n",
    "def run_model(argv = None):\n",
    "    if argv is None:\n",
    "        argv = sys.argv\n",
    "        #input argszw\n",
    "        fasta_file = argv[1]\n",
    "        #e.g. sequences.fa\n",
    "        readout_file = argv[2]\n",
    "        #e.g. wt_readout.dat\n",
    "        parameter_file = argv[3]\n",
    "        #e.g. parameter1.txt\n",
    "\n",
    "    ## excute the code\n",
    "    start_time = time.time()\n",
    "\n",
    "    parameters = train(parameter_file)\n",
    "\n",
    "    cros_eval(parameters,fasta_file,readout_file)\n",
    "\n",
    "    # reports time consumed during execution (secs)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "def create_model(dim_num):\n",
    "\n",
    "    # Modified custom metric functions\n",
    "    def coeff_determination(y_true, y_pred):\n",
    "        SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "        SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "        return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "    def spearman_fn(y_true, y_pred):\n",
    "        return tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32),tf.cast(y_true, tf.float32)], Tout=tf.float32)\n",
    "    \n",
    "    #deepsea arquitecture\n",
    "    model = tf.keras.Sequential()\n",
    "    #First Conv1D\n",
    "    model.add(tf.keras.layers.Conv1D(filters=320, \n",
    "                 kernel_size=8, \n",
    "                 input_shape=(dim_num[1],dim_num[2])))\n",
    "\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=4,strides=4))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.20))\n",
    "    #Second Conv1D\n",
    "    model.add(tf.keras.layers.Conv1D(filters=480, \n",
    "                 kernel_size=8))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=4,strides=4))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.20))\n",
    "    #Third Conv1D\n",
    "    model.add(tf.keras.layers.Conv1D(filters=960, \n",
    "                 kernel_size=8))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.50))\n",
    "    #Dense Layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(925, activation='relu'))\n",
    "    #Output Layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy',coeff_determination, spearman_fn])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def cros_eval(parameters,fasta_file_positive,fasta_file_negative):\n",
    "    # Preprocess the data\n",
    "    positive_control = preprocess(f'./data/{fasta_file_positive}','./wt_readout.dat')\n",
    "    positive_control = prep.one_hot_encode()\n",
    "    positive_control_names = prep.read_fasta_name_into_array()\n",
    "\n",
    "    negative_control = preprocess(f'./data/{fasta_file_negative}','./wt_readout.dat')\n",
    "    negative_control  = prep.one_hot_encode()\n",
    "    negative_control_names = prep.read_fasta_name_into_array()\n",
    "\n",
    "    features = np.append(positive_control['forward'],negative_control['forward'],axis=0)\n",
    "    targets = np.append(np.ones(len(positive_control['forward'])), np.zeros(len(negative_control['forward'])))      \n",
    "    names = np.append(positive_control_names, negative_control_names, axis=0).tolist()\n",
    "\n",
    "\n",
    "\n",
    "    dim_num = fw_fasta.shape\n",
    "\n",
    "\n",
    "    features_shuffle, target_shuffle, names_shuffle = shuffle(features, targets, names, random_state=seed)\n",
    "\n",
    "    target_shuffle = np.array(target_shuffle)\n",
    "\n",
    "    #initialize metrics to save values \n",
    "    metrics = []\n",
    "\n",
    "    #Provides train/test indices to split data in train/test sets.\n",
    "    kFold = StratifiedKFold(n_splits=10)\n",
    "    ln = np.zeros(len(target_shuffle))\n",
    "        \n",
    "    pred_vals = pandas.DataFrame()\n",
    "\n",
    "    Fold=0\n",
    "    model_name = parameters['model_name']\n",
    "\n",
    "    for train, test in kFold.split(ln, ln):\n",
    "        model = None\n",
    "        model = create_model(dim_num)\n",
    "\n",
    "        fwd_train = forward_shuffle[train]\n",
    "        fwd_test = forward_shuffle[test]\n",
    "        \n",
    "\n",
    "        y_train = target_shuffle[train]\n",
    "        y_test = target_shuffle[test]\n",
    "\n",
    "        names_train = names_shuffle[test]\n",
    "        names_test = names_shuffle[test]\n",
    "\n",
    "        history = model.fit(fwd_train, y_train, epochs=parameters['epochs'], batch_size=parameters['batch_size'], validation_split=parameters['validation_split'])\n",
    "        \n",
    "        history2 = model.evaluate(fwd_test, y_test)\n",
    "\n",
    "        pred = model.predict(fwd_test)\n",
    "\n",
    "        metrics.append(history2)\n",
    "\n",
    "        pred = np.reshape(pred,len(pred))\n",
    "\n",
    "        temp = pandas.DataFrame({'sequence_names':np.array(names_test).flatten(),\n",
    "                                     'true_vals':np.array(y_test).flatten(),\n",
    "                                     'pred_vals':np.array(pred).flatten()})\n",
    "        temp['Fold'] = Fold\n",
    "\n",
    "        Fold=Fold+1\n",
    "\n",
    "        pred_vals = pred_vals.append(temp,ignore_index=True)\n",
    "\n",
    "    \n",
    "    pred_vals.to_csv(f'./outs/metrics/{model_name}.csv')\n",
    "\n",
    "    print('[INFO] Calculating 10Fold CV metrics')   \n",
    "    g1 = []\n",
    "    g2 = []\n",
    "    g3 = []\n",
    "    for i in metrics:\n",
    "        loss, r_2, spearman_val = i\n",
    "        g1.append(loss)\n",
    "        g2.append(r_2)\n",
    "        g3.append(spearman_val)\n",
    "\n",
    "    print(g2)\n",
    "    print(g3)\n",
    "    print('seed number = %d' %seed)\n",
    "    print('Mean loss of 10-fold cv is ' + str(np.mean(g1)))\n",
    "    print('Mean R_2 score of 10-fold cv is ' + str(np.mean(g2)))\n",
    "    print('Mean Spearman of 10-fold cv is ' + str(np.mean(g3)))\n",
    "\n",
    "    metrics_dataframe = pandas.DataFrame({\"mean_loss\":[np.mean(g1)],\n",
    "                                          \"R_2\":[np.mean(g2)],\n",
    "                                          \"Spearman\":[np.mean(g3)]})\n",
    "\n",
    "\n",
    "    metrics_dataframe.to_csv(f'./outs/metrics/{model_name}_CV_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-03-09 11:33:49.486729: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-03-09 11:33:49.486910: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-03-09 11:33:49.643305: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd4778c5550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd465211790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd46667a700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd455617670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd465211b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd46529d9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "!python deepsea_classification.py top_10percent.fa bottom_10percent.fa parameters/parameters_deepsea.txt > outs/logs/deepsea.out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}