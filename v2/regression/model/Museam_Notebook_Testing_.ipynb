{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "harmful-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# IMPORT LIBRARIES\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../preprocessing/')\n",
    "from data_preprocess import preprocess\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Tensorflow objects\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D, BatchNormalization, Activation, concatenate, ReLU\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "#from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow import keras\n",
    "from numpy import newaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defined-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suburban-thermal",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "three-audio",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "#Reproducibility\n",
    "seed = 460\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET TRAIN\n",
    "# Get dictionary from text file\n",
    "def train(file_name):\n",
    "    dict = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           dict[key] = val\n",
    "\n",
    "    # change string values to integer values\n",
    "    dict[\"filters\"] = int(dict[\"filters\"])\n",
    "    dict[\"kernel_size\"] = int(dict[\"kernel_size\"])\n",
    "    dict[\"epochs\"] = int(dict[\"epochs\"])\n",
    "    dict[\"batch_size\"] = int(dict[\"batch_size\"])\n",
    "    dict[\"validation_split\"] = float(dict[\"validation_split\"])    \n",
    "    return dict\n",
    "\n",
    "def run_model(argv = None):\n",
    "    if argv is None:\n",
    "        argv = sys.argv\n",
    "        fasta_file = argv[1]\n",
    "        readout = argv[2]\n",
    "        parameter_file = argv[3]\n",
    "    else:\n",
    "        fasta_file = argv[0]\n",
    "        readout = argv[1]\n",
    "        parameter_file = argv[2]\n",
    "\n",
    "    ## excute the code\n",
    "    start_time = time.time()\n",
    "\n",
    "    parameters = train(parameter_file)\n",
    "\n",
    "    cros_eval(parameters,fasta_file,readout)\n",
    "\n",
    "    # reports time consumed during execution (secs)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET UTILS METRICS\n",
    "@tf.function()\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    #print(f'[INFO] SS_tot = {SS_tot}')\n",
    "    #print(f'[INFO] SS_res = {SS_res}')\n",
    "    coeff_det = (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "    #print(f'[INFO] Coeff_det = {coeff_det}')\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "@tf.function()    \n",
    "def spearman_fn(y_true, y_pred):\n",
    "    spearman = tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32),tf.cast(y_true, tf.float32)], Tout=tf.float32)\n",
    "    #print(f'[INFO] Spearman = {spearman}')\n",
    "    return spearman\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET CUSTOM LOSSES\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        \n",
    "@tf.function()\n",
    "def rank_mse(yTrue, yPred):\n",
    "\n",
    "  def calculate_loss(yTrue, yPred):\n",
    "    \n",
    "\n",
    "    print(f'[INFO] Print yTrue: {yTrue}')\n",
    "    print(f'[INFO] Print yPred: {yPred}')\n",
    "    \n",
    "    yTrue = tf.reshape(yTrue,shape=(1,yTrue.shape[0]))\n",
    "    yPred = tf.reshape(yPred,shape=(1,yPred.shape[0]))\n",
    "    \n",
    "    #do\n",
    "    lambda_value=1\n",
    "    size = yTrue.get_shape()[1]\n",
    "    #pass lambda value as tensor\n",
    "    lambda_value = tf.convert_to_tensor(lambda_value,dtype=\"float32\")\n",
    "    #get vector ranks\n",
    "    rank_yTrue = tf.argsort(tf.argsort(yTrue))\n",
    "    rank_yPred = tf.argsort(tf.argsort(yPred))\n",
    "    print(f'[INFO] Print ranked yTrue: {rank_yTrue}')\n",
    "    print(f'[INFO] Print ranked yPred: {rank_yPred}')\n",
    "    #calculate losses\n",
    "\n",
    "    #calculate mse\n",
    "    print(f'\\n[INFO] Calculating normal mse')\n",
    "    mse = tf.subtract(yTrue,yPred)\n",
    "    print(f'[INFO] subtract mse: {mse}')\n",
    "    mse = tf.square(mse)\n",
    "    print(f'[INFO] square mse: {mse}')\n",
    "    mse = tf.math.reduce_sum(mse).numpy()\n",
    "    print(f'[INFO] reduce sum mse: {mse}')\n",
    "    mse = tf.divide(mse,size)\n",
    "    print(f'[INFO] divide by size mse: {mse}')   \n",
    "    mse = tf.cast(mse,dtype=\"float32\")\n",
    "    print(f'[INFO] final mse: {mse}')\n",
    "  \n",
    "    #calculate rank_mse\n",
    "    print(f'\\n[INFO] Calculating rank mse')\n",
    "    rank_mse = tf.cast(tf.subtract(rank_yTrue,rank_yPred),dtype=\"float32\")\n",
    "    print(f'[INFO] substract rank_mse: {rank_mse}')\n",
    "    rank_mse = tf.square(rank_mse)\n",
    "    print(f'[INFO] square rank_mse: {rank_mse}')\n",
    "    rank_mse = tf.math.reduce_sum(rank_mse).numpy()\n",
    "    print(f'[INFO] reduce sum rank_mse: {rank_mse}')\n",
    "    #rank_mse = tf.math.sqrt(rank_mse)\n",
    "    print(f'[INFO] square root rank_mse: {rank_mse}')  \n",
    "    rank_mse = tf.divide(rank_mse,size)\n",
    "    print(f'[INFO] divide by size rank_mse: {rank_mse}') \n",
    "    print(f'[INFO] final rank_mse: {rank_mse}')\n",
    "\n",
    "    #(1 - lambda value)* mse(part a of loss)\n",
    "    loss_a = tf.multiply(tf.subtract(tf.ones(1,dtype=\"float32\"),lambda_value),mse)\n",
    "    print(f'\\n[INFO] Final loss a: {loss_a}')\n",
    "    #lambda value * rank_mse (part b of loss)\n",
    "    loss_b = tf.multiply(lambda_value,rank_mse)\n",
    "    print(f'[INFO] Final loss b: {loss_b}')\n",
    "    #final loss\n",
    "    loss = tf.add(loss_a,loss_b)\n",
    "    print(f'[INFO] Final loss: {loss}')\n",
    "    return loss\n",
    "\n",
    "  debug=False\n",
    "\n",
    "  if not debug:\n",
    "    with HiddenPrints():\n",
    "      loss = calculate_loss(yTrue, yPred)\n",
    "      return loss\n",
    "  else:\n",
    "    loss = calculate_loss(yTrue, yPred)\n",
    "    return loss\n",
    "\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET MODEL CONSTRUCTION\n",
    "class ConvolutionLayer(Conv1D):\n",
    "    def __init__(self, \n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 data_format,\n",
    "                 padding='valid',\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 __name__ = 'ConvolutionLayer',\n",
    "                 **kwargs):\n",
    "                 \n",
    "        super(ConvolutionLayer, self).__init__(filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            **kwargs)\n",
    "        self.run_value = 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "      ## shape of self.kernel is (12, 4, 512)\n",
    "      ##the type of self.kernel is <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "        if self.run_value > 2:\n",
    "\n",
    "            x_tf = self.kernel  ##x_tf after reshaping is a tensor and not a weight variable :(\n",
    "            x_tf = tf.transpose(x_tf, [2, 0, 1])\n",
    "\n",
    "            alpha = 100\n",
    "            beta = 1/alpha\n",
    "            bkg = tf.constant([0.295, 0.205, 0.205, 0.295])\n",
    "            bkg_tf = tf.cast(bkg, tf.float32)\n",
    "            filt_list = tf.map_fn(lambda x:\n",
    "                                  tf.math.scalar_mul(beta, tf.subtract(tf.subtract(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1)),\n",
    "                                  tf.expand_dims(tf.math.log(tf.math.reduce_sum(tf.math.exp(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1))), axis = 1)), axis = 1)),\n",
    "                                  tf.math.log(tf.reshape(tf.tile(bkg_tf, [tf.shape(x)[0]]), [tf.shape(x)[0], tf.shape(bkg_tf)[0]])))), x_tf)\n",
    "            #print(\"type of output from map_fn is\", type(filt_list)) ##type of output from map_fn is <class 'tensorflow.python.framework.ops.Tensor'>   shape of output from map_fn is (10, 12, 4)\n",
    "            #print(\"shape of output from map_fn is\", filt_list.shape)\n",
    "            #transf = tf.reshape(filt_list, [12, 4, self.filters]) ##12, 4, 512\n",
    "            transf = tf.transpose(filt_list, [1, 2, 0])\n",
    "            ##type of transf is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "            outputs = self._convolution_op(inputs, transf) ## type of outputs is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "\n",
    "        else:\n",
    "            outputs = self._convolution_op(inputs, self.kernel)\n",
    "        self.run_value += 1\n",
    "        return outputs\n",
    "\n",
    "class Museam:\n",
    "    def __init__(self,\n",
    "                 dim_num,\n",
    "                 filters, \n",
    "                 kernel_size, \n",
    "                 pool_type, \n",
    "                 regularizer, \n",
    "                 activation_type, \n",
    "                 epochs,\n",
    "                 batch_size, \n",
    "                 loss_func, \n",
    "                 optimizer,\n",
    "                 model_name):\n",
    "\n",
    "        \"\"\"initialize basic parameters\"\"\"\n",
    "        self.dim_num = dim_num\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_type = pool_type\n",
    "        self.regularizer = regularizer\n",
    "        self.activation_type = activation_type\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        dim_num = self.dim_num\n",
    "\n",
    "        # Input Node\n",
    "        forward = tf.keras.Input(shape=(dim_num[1],dim_num[2]), name = 'forward')\n",
    "        reverse = tf.keras.Input(shape=(dim_num[1],dim_num[2]), name = 'reverse')\n",
    "\n",
    "        # Multinomial Layer\n",
    "        first_layer = ConvolutionLayer(filters=self.filters, \n",
    "                                       kernel_size=self.kernel_size, \n",
    "                                       strides=1, \n",
    "                                       data_format='channels_last', \n",
    "                                       use_bias = True)\n",
    "\n",
    "        fw = first_layer(forward)\n",
    "        bw = first_layer(reverse)\n",
    "\n",
    "        # Concatenate both strands\n",
    "        concat = concatenate([fw, bw], axis=1)\n",
    "        pool_size_input = concat.shape[1]\n",
    "        concat_relu = ReLU()(concat)\n",
    "\n",
    "        #Pooling Layer\n",
    "        if self.pool_type == 'Max':\n",
    "            pool_layer = MaxPooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "            #pool_layer = MaxPooling1D(pool_size=12)(concat_relu)\n",
    "        elif self.pool_type == 'Ave':\n",
    "            pool_layer = AveragePooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "        elif self.pool_type == 'custom':\n",
    "            def out_shape(input_shape):\n",
    "                shape = list(input_shape)\n",
    "                print(input_shape)\n",
    "                shape[0] = 10\n",
    "                return tuple(shape)\n",
    "            #model.add(Lambda(top_k, arguments={'k': 10}))\n",
    "            def top_k(inputs, k):\n",
    "                # tf.nn.top_k Finds values and indices of the k largest entries for the last dimension\n",
    "                print(inputs.shape)\n",
    "                inputs2 = tf.transpose(inputs, [0,2,1])\n",
    "                new_vals = tf.nn.top_k(inputs2, k=k, sorted=True).values\n",
    "                # transform back to (None, 10, 512)\n",
    "                return tf.transpose(new_vals, [0,2,1])\n",
    "\n",
    "            pool_layer = Lambda(top_k, arguments={'k': 2})(concat_relu)\n",
    "            pool_layer = AveragePooling1D(pool_size=2)(pool_layer)\n",
    "        elif self.pool_type == 'custom_sum':\n",
    "            ## apply relu function before custom_sum functions\n",
    "            def summed_up(inputs):\n",
    "                #nonzero_vals = tf.keras.backend.relu(inputs)\n",
    "                new_vals = tf.math.reduce_sum(inputs, axis = 1, keepdims = True)\n",
    "                return new_vals\n",
    "            pool_layer = Lambda(summed_up)(concat_relu)\n",
    "        else:\n",
    "            raise NameError('Set the pooling layer name correctly')\n",
    "\n",
    "        # Flatten Layer (None, 512)\n",
    "        flat = Flatten()(pool_layer)\n",
    "        if self.activation_type == 'linear':\n",
    "            if self.regularizer == 'L_1':\n",
    "                outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l1(0.001), activation= self.activation_type)(flat)\n",
    "            elif self.regularizer == 'L_2':\n",
    "                outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation= self.activation_type)(flat)\n",
    "            else:\n",
    "                raise NameError('Set the regularizer name correctly')\n",
    "        elif self.activation_type =='sigmoid':\n",
    "            outputs = Dense(1, activation= self.activation_type)(flat)\n",
    "\n",
    "        # Model Creation\n",
    "        model = keras.Model(inputs=[forward, reverse], outputs=outputs)\n",
    "\n",
    "        ## Model Summary\n",
    "        #model.summary()\n",
    "        #if self.loss_func == 'mse':\n",
    "        #    model.compile(loss='mean_squared_error', optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'huber':\n",
    "        #    loss_huber = keras.losses.Huber(delta=1)\n",
    "        #    model.compile(loss=loss_huber, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'mae':\n",
    "        #    loss_mae = keras.losses.MeanAbsoluteError()\n",
    "        #    model.compile(loss=loss_mae, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'rank_mse':\n",
    "        #    model.compile(loss=rank_mse, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'poisson':\n",
    "        #    poisson_loss = keras.losses.Poisson()\n",
    "        #    model.compile(loss=poisson_loss, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'binary_crossentropy':\n",
    "        #    binary_crossentropy_loss = keras.losses.BinaryCrossentropy()\n",
    "        #    model.compile(loss=binary_crossentropy_loss, optimizer=self.optimizer, metrics = ['binary_accuracy'])\n",
    "        #else:\n",
    "        #    raise NameError('Unrecognized Loss Function')\n",
    "\n",
    "        return model\n",
    "\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# EVAL MODEL\n",
    "\n",
    "def cros_eval(parameters,\n",
    "              fasta_file_positive,\n",
    "              readout):\n",
    "\n",
    "    # Preprocess the data\n",
    "    prep = preprocess(f'../data/{fasta_file_positive}',f'../data/{readout}')\n",
    "    names = prep.read_fasta_name_into_array()\n",
    "    prep_dict = prep.one_hot_encode()\n",
    "    \n",
    "    \n",
    "    fw_fasta = prep_dict[\"forward\"]\n",
    "    rc_fasta = prep_dict[\"reverse\"]\n",
    "    readout =  prep_dict[\"readout\"]\n",
    "    \n",
    "    #print(fw_fasta)\n",
    "    print(readout)\n",
    "\n",
    "    if parameters['activation_type'] == 'linear':\n",
    "        readout = np.log2(readout)\n",
    "        if parameters['scaling'] == None:\n",
    "            readout = np.ndarray.tolist(readout)\n",
    "        elif parameters['scaling'] == \"0_1\":\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaler.fit(readout.reshape(-1, 1))\n",
    "            readout = scaler.transform(readout.reshape(-1, 1))\n",
    "            readout = readout.flatten()\n",
    "            readout = np.ndarray.tolist(readout)\n",
    "      \n",
    "    # Shuffle the data\n",
    "    forward_shuffle, readout_shuffle, names_shuffle = shuffle(fw_fasta, readout, names, random_state=seed)\n",
    "    reverse_shuffle, readout_shuffle, names_shuffle = shuffle(rc_fasta, readout, names, random_state=seed)\n",
    "    readout_shuffle = np.array(readout_shuffle)\n",
    "    \n",
    "    # Get dim\n",
    "    dim_num = forward_shuffle.shape\n",
    "\n",
    "\n",
    "    #initialize metrics to save values \n",
    "    metrics = []\n",
    "\n",
    "    #Provides train/test indices to split data in train/test sets.\n",
    "    kFold = StratifiedKFold(n_splits=10)\n",
    "    ln = np.zeros(len(readout_shuffle))\n",
    "        \n",
    "    pred_vals = pandas.DataFrame()\n",
    "    cv_results =pandas.DataFrame()\n",
    "\n",
    "    Fold=0\n",
    "    model_name = parameters['model_name']\n",
    "\n",
    "    for train, test in kFold.split(ln, ln):\n",
    "        print(f'[INFO] CALCULATING FOLD=={Fold}')\n",
    "\n",
    "        model = Museam(dim_num,\n",
    "                      parameters['filters'], \n",
    "                      parameters['kernel_size'], \n",
    "                      parameters['pool_type'], \n",
    "                      parameters['regularizer'], \n",
    "                      parameters['activation_type'], \n",
    "                      parameters['epochs'],\n",
    "                      parameters['batch_size'], \n",
    "                      parameters['loss_func'], \n",
    "                      parameters['optimizer'],\n",
    "                      parameters['model_name']).create_model()\n",
    "  \n",
    "\n",
    "        # Get splits\n",
    "        fwd_train = forward_shuffle[train]\n",
    "        rc_train = reverse_shuffle[train]\n",
    "        fwd_test = forward_shuffle[test]\n",
    "        rc_test = reverse_shuffle[test]\n",
    "        y_train = readout_shuffle[train]\n",
    "        y_test = readout_shuffle[test]\n",
    "        names_train = names_shuffle[test]\n",
    "        names_test = names_shuffle[test]\n",
    "        \n",
    "        print(fwd_train.shape)\n",
    "        \n",
    "\n",
    "        # Train model\n",
    "        \n",
    "        model.compile(loss='mean_squared_error', \n",
    "                      optimizer=parameters['optimizer'], \n",
    "                      metrics = [coeff_determination, spearman_fn],\n",
    "                      run_eagerly=False)\n",
    "        \n",
    "        model.fit({'forward': fwd_train, 'reverse': rc_train}, \n",
    "                            y_train, \n",
    "                            epochs=20, \n",
    "                            batch_size=parameters['batch_size'], \n",
    "                            validation_split=parameters['validation_split'],\n",
    "                            workers=4\n",
    "                            )\n",
    "        \n",
    "        model.compile(loss=rank_mse, \n",
    "                      optimizer=parameters['optimizer'],\n",
    "                      metrics = [coeff_determination, spearman_fn],\n",
    "                      run_eagerly=True\n",
    "                     )\n",
    "        \n",
    "        model.fit({'forward': fwd_train, 'reverse': rc_train}, \n",
    "                            y_train, \n",
    "                            epochs=5, \n",
    "                            batch_size=2196, \n",
    "                            validation_split=parameters['validation_split']\n",
    "                            )\n",
    "\n",
    "        # Get metrics\n",
    "        loss, R2, Spearman = model.evaluate({'forward': fwd_test, 'reverse': rc_test}, y_test)\n",
    "        pred = model.predict({'forward': fwd_test, 'reverse': rc_test})\n",
    "        pred = np.reshape(pred,len(pred))\n",
    "        \n",
    "\n",
    "        # Temporary fold dataframes\n",
    "        temp = pandas.DataFrame({'sequence_names':np.array(names_test).flatten(),\n",
    "                                     'true_vals':np.array(y_test).flatten(),\n",
    "                                     'pred_vals':np.array(pred).flatten()})                        \n",
    "        temp['Fold'] = Fold\n",
    "\n",
    "        temp2 = pandas.DataFrame({\"Fold\":[Fold],\n",
    "                                  \"R2\":[R2],\n",
    "                                  \"Spearman\":[Spearman],\n",
    "                                  \"Loss\":[loss], \n",
    "        })\n",
    "\n",
    "        Fold=Fold+1\n",
    "        \n",
    "\n",
    "        #append to main dataframe\n",
    "        pred_vals = pred_vals.append(temp,ignore_index=True)\n",
    "        cv_results = cv_results.append(temp2, ignore_index=True)\n",
    "\n",
    "    \n",
    "    pred_vals.to_csv(f'../outs/metrics/{model_name}.csv')\n",
    "\n",
    "\n",
    "    #calculate mean accuracy across all folds\n",
    "    mean_R2 = cv_results['R2'].mean()\n",
    "    mean_spearman = cv_results['Spearman'].mean()\n",
    "    cv_results = cv_results.append({'Fold':'All folds','Loss':'None','Spearman':mean_spearman,'R2':mean_R2}, ignore_index=True)\n",
    "    cv_results.to_csv(f'../outs/metrics/{model_name}_cv_results.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-builder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# RUN SCRIPT\n",
    "\n",
    "#run_model()\n",
    "#nohup python deepsea.py sequences.fa wt_readout.dat parameters. > outs/deepsea.out &\n",
    "\n",
    "\n",
    "#run locally\n",
    "\n",
    "argv = ['sequences.fa','wt_readout.dat','../parameters/parameters_mixed_mse_and_rank_mse.txt']\n",
    "run_model(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# RUN SCRIPT\n",
    "\n",
    "#run_model()\n",
    "#nohup python deepsea.py sequences.fa wt_readout.dat parameters. > outs/deepsea.out &\n",
    "\n",
    "#run locally\n",
    "\n",
    "argv = ['silencer_sequences.fa','silencer_readout.dat','../parameters/parameters_test.txt']\n",
    "run_model(argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('museam_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "e1b2108af56e0534cb60fc6f36fc8b9d20f5606f6a579c5ffad689c3e959bd13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}