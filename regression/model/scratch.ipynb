{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D, BatchNormalization, Activation, concatenate, ReLU\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_preprocess import preprocess\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from numpy import newaxis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#Reproducibility\n",
    "seed = 460\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()\n",
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def rank_mse(yTrue, yPred):\n",
    "\n",
    "  def calculate_loss(yTrue, yPred):\n",
    "    \n",
    "\n",
    "    print(f'[INFO] Print yTrue: {yTrue}')\n",
    "    print(f'[INFO] Print yPred: {yPred}')\n",
    "    \n",
    "    yTrue = tf.reshape(yTrue,shape=(1,yTrue.shape[0]))\n",
    "    yPred = tf.reshape(yPred,shape=(1,yPred.shape[0]))\n",
    "    \n",
    "    #do\n",
    "    lambda_value=0.5\n",
    "    size = yTrue.get_shape()[1]\n",
    "    #pass lambda value as tensor\n",
    "    lambda_value = tf.convert_to_tensor(lambda_value,dtype=\"float32\")\n",
    "    #get vector ranks\n",
    "    rank_yTrue = tf.argsort(tf.argsort(yTrue))\n",
    "    rank_yPred = tf.argsort(tf.argsort(yPred))\n",
    "    print(f'[INFO] Print ranked yTrue: {rank_yTrue}')\n",
    "    print(f'[INFO] Print ranked yPred: {rank_yPred}')\n",
    "    #calculate losses\n",
    "\n",
    "    #calculate mse\n",
    "    print(f'\\n[INFO] Calculating normal mse')\n",
    "    mse = tf.subtract(yTrue,yPred)\n",
    "    print(f'[INFO] subtract mse: {mse}')\n",
    "    mse = tf.square(mse)\n",
    "    print(f'[INFO] square mse: {mse}')\n",
    "    mse = tf.math.reduce_sum(mse).numpy()\n",
    "    print(f'[INFO] reduce sum mse: {mse}')\n",
    "    mse = tf.divide(mse,size)\n",
    "    print(f'[INFO] divide by size mse: {mse}')   \n",
    "    mse = tf.cast(mse,dtype=\"float32\")\n",
    "    print(f'[INFO] final mse: {mse}')\n",
    "  \n",
    "    #calculate rank_mse\n",
    "    print(f'\\n[INFO] Calculating rank mse')\n",
    "    rank_mse = tf.cast(tf.subtract(rank_yTrue,rank_yPred),dtype=\"float32\")\n",
    "    print(f'[INFO] substract rank_mse: {rank_mse}')\n",
    "    rank_mse = tf.square(rank_mse)\n",
    "    print(f'[INFO] square rank_mse: {rank_mse}')\n",
    "    rank_mse = tf.math.reduce_sum(rank_mse).numpy()\n",
    "    print(f'[INFO] reduce sum rank_mse: {rank_mse}')\n",
    "    rank_mse = tf.math.sqrt(rank_mse)\n",
    "    print(f'[INFO] square root rank_mse: {rank_mse}')  \n",
    "    rank_mse = tf.divide(rank_mse,size)\n",
    "    print(f'[INFO] divide by size rank_mse: {rank_mse}') \n",
    "    print(f'[INFO] final rank_mse: {rank_mse}')\n",
    "\n",
    "    #(1 - lambda value)* mse(part a of loss)\n",
    "    loss_a = tf.multiply(tf.subtract(tf.ones(1,dtype=\"float32\"),lambda_value),mse)\n",
    "    print(f'\\n[INFO] Final loss a: {loss_a}')\n",
    "    #lambda value * rank_mse (part b of loss)\n",
    "    loss_b = tf.multiply(lambda_value,rank_mse)\n",
    "    print(f'[INFO] Final loss b: {loss_b}')\n",
    "    #final loss\n",
    "    loss = tf.add(loss_a,loss_b)\n",
    "    print(f'[INFO] Final loss: {loss}')\n",
    "    return loss\n",
    "\n",
    "  debug=False\n",
    "\n",
    "  if not debug:\n",
    "    with HiddenPrints():\n",
    "      loss = calculate_loss(yTrue, yPred)\n",
    "      return loss\n",
    "  else:\n",
    "    loss = calculate_loss(yTrue, yPred)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]], shape=(4, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Test Rank MSE\n",
    "\n",
    "np.random.seed(10) \n",
    "\n",
    "y = np.array([[1],[2],[3],[4]])\n",
    "y = tf.convert_to_tensor(y,dtype='float32')\n",
    "print(y)\n",
    "\n",
    "y_p = np.array([[2],[0],[3],[4]])\n",
    "y_p = tf.convert_to_tensor(y_p,dtype='float32')\n",
    "print(y_p)\n",
    "\n",
    "loss = rank_mse(y,y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(file_name):\n",
    "    dict = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           dict[key] = val\n",
    "\n",
    "    # change string values to integer values\n",
    "    dict[\"filters\"] = int(dict[\"filters\"])\n",
    "    dict[\"kernel_size\"] = int(dict[\"kernel_size\"])\n",
    "    dict[\"epochs\"] = int(dict[\"epochs\"])\n",
    "    dict[\"batch_size\"] = int(dict[\"batch_size\"])\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(Conv1D):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 data_format,\n",
    "                 padding='valid',\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 __name__ = 'ConvolutionLayer',\n",
    "                 **kwargs):\n",
    "        super(ConvolutionLayer, self).__init__(filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            **kwargs)\n",
    "        self.run_value = 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "      ## shape of self.kernel is (12, 4, 512)\n",
    "      ##the type of self.kernel is <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "        if self.run_value > 2:\n",
    "\n",
    "            x_tf = self.kernel  ##x_tf after reshaping is a tensor and not a weight variable :(\n",
    "            x_tf = tf.transpose(x_tf, [2, 0, 1])\n",
    "\n",
    "            alpha = 100\n",
    "            beta = 1/alpha\n",
    "            bkg = tf.constant([0.295, 0.205, 0.205, 0.295])\n",
    "            bkg_tf = tf.cast(bkg, tf.float32)\n",
    "            filt_list = tf.map_fn(lambda x:\n",
    "                                  tf.math.scalar_mul(beta, tf.subtract(tf.subtract(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1)),\n",
    "                                  tf.expand_dims(tf.math.log(tf.math.reduce_sum(tf.math.exp(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1))), axis = 1)), axis = 1)),\n",
    "                                  tf.math.log(tf.reshape(tf.tile(bkg_tf, [tf.shape(x)[0]]), [tf.shape(x)[0], tf.shape(bkg_tf)[0]])))), x_tf)\n",
    "            #print(\"type of output from map_fn is\", type(filt_list)) ##type of output from map_fn is <class 'tensorflow.python.framework.ops.Tensor'>   shape of output from map_fn is (10, 12, 4)\n",
    "            #print(\"shape of output from map_fn is\", filt_list.shape)\n",
    "            #transf = tf.reshape(filt_list, [12, 4, self.filters]) ##12, 4, 512\n",
    "            transf = tf.transpose(filt_list, [1, 2, 0])\n",
    "            ##type of transf is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "            outputs = self._convolution_op(inputs, transf) ## type of outputs is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "\n",
    "        else:\n",
    "            outputs = self._convolution_op(inputs, self.kernel)\n",
    "        self.run_value += 1\n",
    "        return outputs\n",
    "\n",
    "class nn_model:\n",
    "    def __init__(self, fasta_file, readout_file, filters, kernel_size, pool_type, regularizer, activation_type, epochs, batch_size, loss_func, optimizer,scaling,model_name):\n",
    "        \"\"\"initialize basic parameters\"\"\"\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_type = pool_type\n",
    "        self.regularizer = regularizer\n",
    "        self.activation_type = activation_type\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.fasta_file = fasta_file\n",
    "        self.readout_file = readout_file\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "        self.scaling = scaling\n",
    "        self.model_name = model_name\n",
    "\n",
    "        #self.eval()\n",
    "        self.cross_val()\n",
    "        #self.cross_val_binning()\n",
    "\n",
    "    def create_model(self):\n",
    "        # different metric functions\n",
    "        def coeff_determination(y_true, y_pred):\n",
    "            SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "            SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "            return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "        def spearman_fn(y_true, y_pred):\n",
    "            return tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32),\n",
    "                   tf.cast(y_true, tf.float32)], Tout=tf.float32)\n",
    "\n",
    "        # building model\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "        # if want mono-nucleotide sequences\n",
    "        dict = prep.one_hot_encode()\n",
    "        # if want dinucleotide sequences\n",
    "        #dict = prep.dinucleotide_encode()\n",
    "\n",
    "        readout = dict[\"readout\"]\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "\n",
    "        dim_num = fw_fasta.shape\n",
    "\n",
    "        # To build this model with the functional API,\n",
    "        # you would start by creating an input node:\n",
    "        forward = keras.Input(shape=(dim_num[1],dim_num[2]), name = 'forward')\n",
    "        reverse = keras.Input(shape=(dim_num[1],dim_num[2]), name = 'reverse')\n",
    "\n",
    "        #first_layer = Conv1D(filters=self.filters, kernel_size=self.kernel_size, data_format='channels_last', input_shape=(dim_num[1],dim_num[2]), use_bias = True)\n",
    "        first_layer = ConvolutionLayer(filters=self.filters, kernel_size=self.kernel_size, strides=1, data_format='channels_last', use_bias = True)\n",
    "\n",
    "        fw = first_layer(forward)\n",
    "        bw = first_layer(reverse)\n",
    "\n",
    "        concat = concatenate([fw, bw], axis=1)\n",
    "        pool_size_input = concat.shape[1]\n",
    "\n",
    "        concat_relu = ReLU()(concat)\n",
    "\n",
    "        if self.pool_type == 'Max':\n",
    "            pool_layer = MaxPooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "            #pool_layer = MaxPooling1D(pool_size=12)(concat_relu)\n",
    "        elif self.pool_type == 'Ave':\n",
    "            pool_layer = AveragePooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "        elif self.pool_type == 'custom':\n",
    "            def out_shape(input_shape):\n",
    "                shape = list(input_shape)\n",
    "                print(input_shape)\n",
    "                shape[0] = 10\n",
    "                return tuple(shape)\n",
    "            #model.add(Lambda(top_k, arguments={'k': 10}))\n",
    "            def top_k(inputs, k):\n",
    "                # tf.nn.top_k Finds values and indices of the k largest entries for the last dimension\n",
    "                print(inputs.shape)\n",
    "                inputs2 = tf.transpose(inputs, [0,2,1])\n",
    "                new_vals = tf.nn.top_k(inputs2, k=k, sorted=True).values\n",
    "                # transform back to (None, 10, 512)\n",
    "                return tf.transpose(new_vals, [0,2,1])\n",
    "\n",
    "            pool_layer = Lambda(top_k, arguments={'k': 2})(concat_relu)\n",
    "            pool_layer = AveragePooling1D(pool_size=2)(pool_layer)\n",
    "        elif self.pool_type == 'custom_sum':\n",
    "            ## apply relu function before custom_sum functions\n",
    "            def summed_up(inputs):\n",
    "                #nonzero_vals = tf.keras.backend.relu(inputs)\n",
    "                new_vals = tf.math.reduce_sum(inputs, axis = 1, keepdims = True)\n",
    "                return new_vals\n",
    "            pool_layer = Lambda(summed_up)(concat_relu)\n",
    "\n",
    "        else:\n",
    "            raise NameError('Set the pooling layer name correctly')\n",
    "\n",
    "        #layer = Conv1D(filters=128, kernel_size=12)(pool_layer)\n",
    "        #layer = Dense(16)(pool_layer)\n",
    "        #pool_size_input = layer.shape[1]\n",
    "        #layer = MaxPooling1D(pool_size=pool_size_input)(layer)\n",
    "\n",
    "        # flatten the layer (None, 512)\n",
    "        flat = Flatten()(pool_layer)\n",
    "\n",
    "        if self.activation_type == 'linear':\n",
    "            if self.regularizer == 'L_1':\n",
    "                outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l1(0.001), activation= self.activation_type)(flat)\n",
    "            elif self.regularizer == 'L_2':\n",
    "                outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation= self.activation_type)(flat)\n",
    "            else:\n",
    "                raise NameError('Set the regularizer name correctly')\n",
    "        elif self.activation_type =='sigmoid':\n",
    "            outputs = Dense(1, activation= self.activation_type)(flat)\n",
    "\n",
    "\n",
    "        model = keras.Model(inputs=[forward, reverse], outputs=outputs)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        if self.loss_func == 'mse':\n",
    "            model.compile(loss='mean_squared_error', optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        elif self.loss_func == 'huber':\n",
    "            loss_huber = keras.losses.Huber(delta=1)\n",
    "            model.compile(loss=loss_huber, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        elif self.loss_func == 'mae':\n",
    "            loss_mae = keras.losses.MeanAbsoluteError()\n",
    "            model.compile(loss=loss_mae, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        elif self.loss_func == 'rank_mse':\n",
    "            model.compile(loss=rank_mse, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        elif self.loss_func == 'poisson':\n",
    "            poisson_loss = keras.losses.Poisson()\n",
    "            model.compile(loss=poisson_loss, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        else:\n",
    "            raise NameError('Unrecognized Loss Function')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def eval(self):\n",
    "\n",
    "        # Preprocess the data to one-hot encoded vector\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "\n",
    "        dict = prep.one_hot_encode()\n",
    "\n",
    "        # if want dinucleotide sequences\n",
    "        # dict = prep.dinucleotide_encode()\n",
    "\n",
    "        # print maximum length without truncation\n",
    "        np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "        readout = dict[\"readout\"]\n",
    "\n",
    "        if self.activation_type == 'linear':\n",
    "\n",
    "            readout = np.log2(readout)\n",
    "\n",
    "            if self.scaling == None:\n",
    "                readout = np.ndarray.tolist(readout)\n",
    "            elif self.scaling == \"0_1\":\n",
    "                scaler = MinMaxScaler(feature_range=(0,1))\n",
    "                scaler.fit(readout.reshape(-1, 1))\n",
    "                readout = scaler.transform(readout.reshape(-1, 1))\n",
    "                readout = readout.flatten()\n",
    "                readout = np.ndarray.tolist(readout)\n",
    "\n",
    "        # 90% Train, 10% Test\n",
    "        x1_train, x1_test, y1_train, y1_test = train_test_split(fw_fasta, readout, test_size=0.1, random_state=seed)\n",
    "        x2_train, x2_test, y2_train, y2_test = train_test_split(rc_fasta, readout, test_size=0.1, random_state=seed)\n",
    "\n",
    "        model = self.create_model()\n",
    "\n",
    "        # change from list to numpy array\n",
    "        y1_train = np.asarray(y1_train)\n",
    "        y1_test = np.asarray(y1_test)\n",
    "        y2_train = np.asarray(y2_train)\n",
    "        y2_test = np.asarray(y2_test)\n",
    "\n",
    "\n",
    "        # Without early stopping\n",
    "        #history = model.fit({'forward': x1_train, 'reverse': x2_train}, y1_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.1)\n",
    "\n",
    "        # Early stopping\n",
    "        #callback = EarlyStopping(monitor='loss', min_delta=0.001, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "        callback = EarlyStopping(monitor='val_spearman_fn', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "        \n",
    "        history = model.fit({'forward': x1_train, 'reverse': x2_train}, y1_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.1, callbacks = [callback])\n",
    "         \n",
    "        history2 = model.evaluate({'forward': x1_test, 'reverse': x2_test}, y1_test)\n",
    "        pred = model.predict({'forward': x1_test, 'reverse': x2_test})\n",
    "\n",
    "        #viz_prediction(pred, y1_test, '{} regression model'.format(self.loss_func), '{}2.png'.format(self.loss_func))\n",
    "\n",
    "        print(\"Seed number is {}\".format(seed))\n",
    "        print('metric values of model.evaluate: '+ str(history2))\n",
    "        print('metrics names are ' + str(model.metrics_names))\n",
    "        \n",
    "        \n",
    "\n",
    "    def cross_val(self):\n",
    "        # Preprocess the data\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "        dict = prep.one_hot_encode()\n",
    "        \n",
    "        # If want dinucleotide sequences\n",
    "        #dict = prep.dinucleotide_encode()\n",
    "\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "        readout = dict[\"readout\"]\n",
    "        names = prep.read_fasta_name_into_array()\n",
    "\n",
    "        if self.activation_type == 'linear':\n",
    "            readout = np.log2(readout)\n",
    "\n",
    "            if self.scaling == 'no_scaling':\n",
    "                readout = np.ndarray.tolist(readout)\n",
    "            elif self.scaling == \"0_1\":\n",
    "                scaler = MinMaxScaler(feature_range=(0,1))\n",
    "                scaler.fit(readout.reshape(-1, 1))\n",
    "                readout = scaler.transform(readout.reshape(-1, 1))\n",
    "                readout = readout.flatten()\n",
    "                readout = np.ndarray.tolist(readout)\n",
    "            elif self.scaling == \"-1_1\":\n",
    "                scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "                scaler.fit(readout.reshape(-1, 1))\n",
    "                readout = scaler.transform(readout.reshape(-1, 1))\n",
    "                readout = readout.flatten()\n",
    "                readout = np.ndarray.tolist(readout)\n",
    "\n",
    "\n",
    "\n",
    "        forward_shuffle, readout_shuffle, names_shuffle = shuffle(fw_fasta, readout, names, random_state=seed)\n",
    "        reverse_shuffle, readout_shuffle, names_shuffle = shuffle(rc_fasta, readout, names, random_state=seed)\n",
    "        readout_shuffle = np.array(readout_shuffle)\n",
    "\n",
    "        # initialize metrics to save values \n",
    "        metrics = []\n",
    "\n",
    "        # Provides train/test indices to split data in train/test sets.\n",
    "        kFold = StratifiedKFold(n_splits=1)\n",
    "        ln = np.zeros(len(readout_shuffle))\n",
    "        \n",
    "        pred_vals = pandas.DataFrame()\n",
    "\n",
    "        Fold=0\n",
    "\n",
    "        for train, test in kFold.split(ln, ln):\n",
    "            model = None\n",
    "            model = self.create_model()\n",
    "\n",
    "            fwd_train = forward_shuffle[train]\n",
    "            fwd_test = forward_shuffle[test]\n",
    "            rc_train = reverse_shuffle[train]\n",
    "            rc_test = reverse_shuffle[test]\n",
    "            y_train = readout_shuffle[train]\n",
    "            y_test = readout_shuffle[test]\n",
    "            names_train = names_shuffle[test]\n",
    "            names_test = names_shuffle[test]\n",
    "\n",
    "            # Early stopping\n",
    "            #callback = EarlyStopping(monitor='loss', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #callback = EarlyStopping(monitor='val_spearman_fn', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0, callbacks = [callback])\n",
    "\n",
    "            history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0)\n",
    "            history2 = model.evaluate({'forward': fwd_test, 'reverse': rc_test}, y_test)\n",
    "            pred = model.predict({'forward': fwd_test, 'reverse': rc_test})\n",
    "\n",
    "            metrics.append(history2)\n",
    "            pred = np.reshape(pred,len(pred))\n",
    "\n",
    "            temp = pandas.DataFrame({'sequence_names':np.array(names_test).flatten(),\n",
    "                                         'true_vals':np.array(y_test).flatten(),\n",
    "                                         'pred_vals':np.array(pred).flatten()})\n",
    "            temp['Fold'] = Fold\n",
    "\n",
    "            Fold=Fold+1\n",
    "\n",
    "            pred_vals = pred_vals.append(temp,ignore_index=True)\n",
    "\n",
    "        pred_vals.to_csv(f'./outs/{self.model_name}.csv')\n",
    "\n",
    "        print('[INFO] Calculating 10Fold CV metrics')   \n",
    "        g1 = []\n",
    "        g2 = []\n",
    "        g3 = []\n",
    "        for i in metrics:\n",
    "            loss, r_2, spearman_val = i\n",
    "            g1.append(loss)\n",
    "            g2.append(r_2)\n",
    "            g3.append(spearman_val)\n",
    "\n",
    "        print(g2)\n",
    "        print(g3)\n",
    "        print('seed number = %d' %seed)\n",
    "        print('Mean loss of 10-fold cv is ' + str(np.mean(g1)))\n",
    "        print('Mean R_2 score of 10-fold cv is ' + str(np.mean(g2)))\n",
    "        print('Mean Spearman of 10-fold cv is ' + str(np.mean(g3)))\n",
    "\n",
    "        metrics_dataframe = pandas.DataFrame({\"mean_loss\":[np.mean(g1)],\n",
    "                                          \"R_2\":[np.mean(g2)],\n",
    "                                          \"Spearman\":[np.mean(g3)]})\n",
    "\n",
    "\n",
    "        metrics_dataframe.to_csv(f'./outs/{self.model_name}_CV_metrics.csv')\n",
    "\n",
    "    def cross_val_binning(self):\n",
    "        # Preprocess the data\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "        dict = prep.one_hot_encode()\n",
    "        # If want dinucleotide sequences\n",
    "        #dict = prep.dinucleotide_encode()\n",
    "\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "        readout = dict[\"readout\"]\n",
    "\n",
    "        if self.activation_type == 'linear':\n",
    "            readout = np.log2(readout)\n",
    "\n",
    "        # Returns the indices that would sort an array.\n",
    "        x = np.argsort(readout)\n",
    "        ind = []\n",
    "\n",
    "        num = 10\n",
    "        for i in range(num):\n",
    "            for j in range(int(len(x)/num)):\n",
    "                id = i + j * num\n",
    "                ind.append(x[id])\n",
    "\n",
    "        forward_bin = fw_fasta[ind]\n",
    "        reverse_bin = rc_fasta[ind]\n",
    "        readout_bin = readout[ind]\n",
    "        #readout_bin = np.array(readout_bin)\n",
    "\n",
    "        # initialize metrics to save values\n",
    "        metrics = []\n",
    "\n",
    "        # Provides train/test indices to split data in train/test sets.\n",
    "        kFold = StratifiedKFold(n_splits=10)\n",
    "        ln = np.zeros(len(readout_bin))\n",
    "        true_vals = []\n",
    "        pred_vals = []\n",
    "\n",
    "        for train, test in kFold.split(ln, ln):\n",
    "            model = None\n",
    "            model = self.create_model()\n",
    "\n",
    "            fwd_train = forward_bin[train]\n",
    "            fwd_test = forward_bin[test]\n",
    "            rc_train = reverse_bin[train]\n",
    "            rc_test = reverse_bin[test]\n",
    "            y_train = readout_bin[train]\n",
    "            y_test = readout_bin[test]\n",
    "\n",
    "            # Early stopping\n",
    "            #callback = EarlyStopping(monitor='loss', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #callback = EarlyStopping(monitor='val_spearman_fn', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0, callbacks = [callback])\n",
    "\n",
    "            history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0)\n",
    "            history2 = model.evaluate({'forward': fwd_test, 'reverse': rc_test}, y_test)\n",
    "            pred = model.predict({'forward': fwd_test, 'reverse': rc_test})\n",
    "\n",
    "            metrics.append(history2)\n",
    "            pred = np.reshape(pred,len(pred))\n",
    "            true_vals.append(y_test.tolist())\n",
    "            pred_vals.append(pred.tolist())\n",
    "\n",
    "        g1 = []\n",
    "        g2 = []\n",
    "        g3 = []\n",
    "        for i in metrics:\n",
    "            loss, r_2, spearman_val = i\n",
    "            g1.append(loss)\n",
    "            g2.append(r_2)\n",
    "            g3.append(spearman_val)\n",
    "\n",
    "        #np.savetxt('true_vals.txt', true_vals)\n",
    "        #np.savetxt('pred_vals.txt', pred_vals)\n",
    "        #viz_prediction(pred_vals, true_vals, '{} delta=1 regression model (seed=460)'.format(self.loss_func), '{}_d1.png'.format(self.loss_func))\n",
    "\n",
    "        print(g2)\n",
    "        print(g3)\n",
    "        print('seed number = %d' %seed)\n",
    "        print('Mean loss of 10-fold cv is ' + str(np.mean(g1)))\n",
    "        print('Mean R_2 score of 10-fold cv is ' + str(np.mean(g2)))\n",
    "        print('Mean Spearman of 10-fold cv is ' + str(np.mean(g3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filters': 512,\n",
       " 'kernel_size': 12,\n",
       " 'pool_type': 'Max',\n",
       " 'regularizer': 'L_1',\n",
       " 'activation_type': 'linear',\n",
       " 'epochs': 30,\n",
       " 'batch_size': 59,\n",
       " 'loss_func': 'rank_mse',\n",
       " 'optimizer': 'adam',\n",
       " 'scaling': '0_1',\n",
       " 'model_name': 'rank_mse_0_1'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_dict = train('./parameters_rank_mse.txt')\n",
    "parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocess('./sequences.fa', './wt_readout.dat')\n",
    "dict = prep.one_hot_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocess('./sequences.fa', './wt_readout.dat')\n",
    "dict = prep.one_hot_encode()\n",
    "# If want dinucleotide sequences\n",
    "#dict = prep.dinucleotide_encode()\n",
    "\n",
    "fw_fasta = dict[\"forward\"]\n",
    "rc_fasta = dict[\"reverse\"]\n",
    "readout = dict[\"readout\"]\n",
    "\n",
    "\n",
    "readout = np.log2(readout)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(readout.reshape(-1, 1))\n",
    "readout = scaler.transform(readout.reshape(-1, 1))\n",
    "readout = readout.flatten()\n",
    "readout = np.ndarray.tolist(readout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2440, 171, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_num = fw_fasta.shape\n",
    "dim_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>A:HNF4A-CHMOD_CHR10:11917871-11917984_[CHR10:11917842-11918013]',\n",
       "       '>A:HNF4A-CHMOD_CHR10:34165653-34165745_[CHR10:34165613-34165784]',\n",
       "       '>A:HNF4A-CHMOD_CHR10:52009954-52010059_[CHR10:52009921-52010092]',\n",
       "       ...,\n",
       "       '>R:HNF4A-NOMOD_CHR9:89020204-89020327_[CHR9:89020180-89020351]',\n",
       "       '>R:HNF4A-NOMOD_CHR9:98508868-98508933_[CHR9:98508815-98508986]',\n",
       "       '>R:HNF4A-NOMOD_CHRY:18213828-18213963_[CHRY:18213810-18213981]'],\n",
       "      dtype='<U309')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_fasta_into_list():\n",
    "        all_seqs = []\n",
    "        with open('./sequences.fa', \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.upper()\n",
    "                if line[0] == '>':\n",
    "                    continue\n",
    "                else:\n",
    "                    all_seqs.append(line)\n",
    "        return all_seqs\n",
    "\n",
    "def read_fasta_name_into_array():\n",
    "        all_seqs = []\n",
    "        with open('./sequences.fa', \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.upper()\n",
    "                if line[0] == '>':\n",
    "                    all_seqs.append(line)\n",
    "                    #continue\n",
    "                else:\n",
    "                    #all_seqs.append(line)\n",
    "                    pass\n",
    "                \n",
    "        return np.array(all_seqs)\n",
    "\n",
    "read_fasta_name_into_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "forward (InputLayer)            [(None, 171, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reverse (InputLayer)            [(None, 171, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "convolution_layer_1 (Convolutio (None, 160, 512)     25088       forward[0][0]                    \n",
      "                                                                 reverse[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 320, 512)     0           convolution_layer_1[0][0]        \n",
      "                                                                 convolution_layer_1[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 320, 512)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 512)       0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 25,601\n",
      "Trainable params: 25,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2196 samples\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "  59/2196 [..............................] - ETA: 1:00 - loss: 1.2442 - coeff_determination: -1.9282 - spearman_fn: 0.4210WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 118/2196 [>.............................] - ETA: 54s - loss: 1.3320 - coeff_determination: -2.1182 - spearman_fn: 0.3262 WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 177/2196 [=>............................] - ETA: 53s - loss: 1.3798 - coeff_determination: -2.0597 - spearman_fn: 0.2734WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 236/2196 [==>...........................] - ETA: 51s - loss: 1.4180 - coeff_determination: -2.1163 - spearman_fn: 0.2308WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 295/2196 [===>..........................] - ETA: 50s - loss: 1.4314 - coeff_determination: -2.1046 - spearman_fn: 0.2187WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 354/2196 [===>..........................] - ETA: 48s - loss: 1.4546 - coeff_determination: -2.0694 - spearman_fn: 0.1904WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 413/2196 [====>.........................] - ETA: 46s - loss: 1.4555 - coeff_determination: -2.1461 - spearman_fn: 0.1889WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 472/2196 [=====>........................] - ETA: 44s - loss: 1.4642 - coeff_determination: -2.1707 - spearman_fn: 0.1802WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 531/2196 [======>.......................] - ETA: 43s - loss: 1.4745 - coeff_determination: -2.2038 - spearman_fn: 0.1673WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 590/2196 [=======>......................] - ETA: 41s - loss: 1.4932 - coeff_determination: -2.4020 - spearman_fn: 0.1441WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 649/2196 [=======>......................] - ETA: 39s - loss: 1.4910 - coeff_determination: -2.4095 - spearman_fn: 0.1476WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 708/2196 [========>.....................] - ETA: 37s - loss: 1.4935 - coeff_determination: -2.3873 - spearman_fn: 0.1440WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 767/2196 [=========>....................] - ETA: 36s - loss: 1.4896 - coeff_determination: -2.3434 - spearman_fn: 0.1493WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 826/2196 [==========>...................] - ETA: 35s - loss: 1.4916 - coeff_determination: -2.3660 - spearman_fn: 0.1474WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 885/2196 [===========>..................] - ETA: 34s - loss: 1.4948 - coeff_determination: -2.3979 - spearman_fn: 0.1436WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 944/2196 [===========>..................] - ETA: 32s - loss: 1.4927 - coeff_determination: -2.4070 - spearman_fn: 0.1462WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1003/2196 [============>.................] - ETA: 31s - loss: 1.5028 - coeff_determination: -2.4881 - spearman_fn: 0.1331WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1062/2196 [=============>................] - ETA: 29s - loss: 1.5069 - coeff_determination: -2.4927 - spearman_fn: 0.1285WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1121/2196 [==============>...............] - ETA: 28s - loss: 1.5114 - coeff_determination: -2.5163 - spearman_fn: 0.1232WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1180/2196 [===============>..............] - ETA: 26s - loss: 1.5179 - coeff_determination: -2.5378 - spearman_fn: 0.1148WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1239/2196 [===============>..............] - ETA: 25s - loss: 1.5225 - coeff_determination: -2.5256 - spearman_fn: 0.1094WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1298/2196 [================>.............] - ETA: 23s - loss: 1.5195 - coeff_determination: -2.5237 - spearman_fn: 0.1126WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1357/2196 [=================>............] - ETA: 21s - loss: 1.5158 - coeff_determination: -2.4997 - spearman_fn: 0.1172WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1416/2196 [==================>...........] - ETA: 20s - loss: 1.5089 - coeff_determination: -2.5075 - spearman_fn: 0.1250WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1475/2196 [===================>..........] - ETA: 18s - loss: 1.4990 - coeff_determination: -2.4739 - spearman_fn: 0.1361WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1534/2196 [===================>..........] - ETA: 17s - loss: 1.5038 - coeff_determination: -2.4666 - spearman_fn: 0.1303WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1593/2196 [====================>.........] - ETA: 15s - loss: 1.5031 - coeff_determination: -2.4976 - spearman_fn: 0.1310WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1652/2196 [=====================>........] - ETA: 14s - loss: 1.5107 - coeff_determination: -2.5576 - spearman_fn: 0.1211WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1711/2196 [======================>.......] - ETA: 12s - loss: 1.5147 - coeff_determination: -2.5745 - spearman_fn: 0.1161WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1770/2196 [=======================>......] - ETA: 11s - loss: 1.5180 - coeff_determination: -2.5928 - spearman_fn: 0.1116WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1829/2196 [=======================>......] - ETA: 9s - loss: 1.5142 - coeff_determination: -2.5906 - spearman_fn: 0.1160 WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1888/2196 [========================>.....] - ETA: 7s - loss: 1.5173 - coeff_determination: -2.5602 - spearman_fn: 0.1123WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "1947/2196 [=========================>....] - ETA: 6s - loss: 1.5218 - coeff_determination: -2.5491 - spearman_fn: 0.1064WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "2006/2196 [==========================>...] - ETA: 4s - loss: 1.5259 - coeff_determination: -2.5726 - spearman_fn: 0.1011WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "2065/2196 [===========================>..] - ETA: 3s - loss: 1.5280 - coeff_determination: -2.5597 - spearman_fn: 0.0985WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "2124/2196 [============================>.] - ETA: 1s - loss: 1.5249 - coeff_determination: -2.5641 - spearman_fn: 0.1020WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "2183/2196 [============================>.] - ETA: 0s - loss: 1.5280 - coeff_determination: -2.5821 - spearman_fn: 0.0976WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "2196/2196 [==============================] - 57s 26ms/sample - loss: 1.5238 - coeff_determination: -2.5889 - spearman_fn: 0.0923\n",
      "Epoch 2/30\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      "  59/2196 [..............................] - ETA: 53s - loss: 1.5414 - coeff_determination: -2.6724 - spearman_fn: 0.0901WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 118/2196 [>.............................] - ETA: 51s - loss: 1.4832 - coeff_determination: -2.9114 - spearman_fn: 0.1482WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 177/2196 [=>............................] - ETA: 52s - loss: 1.5273 - coeff_determination: -2.6194 - spearman_fn: 0.0919WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 236/2196 [==>...........................] - ETA: 50s - loss: 1.5265 - coeff_determination: -2.7336 - spearman_fn: 0.0932WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer_1_1/kernel:0', 'convolution_layer_1_1/bias:0', 'dense_1/bias:0'] when minimizing the loss.\n",
      " 354/2196 [===>..........................] - ETA: 41s - loss: 1.5372 - coeff_determination: -2.7650 - spearman_fn: 0.0809"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-180b2f113a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         model_name='test')\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-6bc84cfd6e47>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fasta_file, readout_file, filters, kernel_size, pool_type, regularizer, activation_type, epochs, batch_size, loss_func, optimizer, scaling, model_name)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#self.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m#self.cross_val_binning()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-6bc84cfd6e47>\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;31m#history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0, callbacks = [callback])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfwd_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reverse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrc_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfwd_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reverse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrc_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfwd_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reverse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrc_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 73\u001b[0;31m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[1;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 760\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2132\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-6bc84cfd6e47>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                   tf.expand_dims(tf.math.log(tf.math.reduce_sum(tf.math.exp(tf.subtract(tf.math.scalar_mul(alpha, x),\n\u001b[1;32m     36\u001b[0m                                   tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1))), axis = 1)), axis = 1)),\n\u001b[0;32m---> 37\u001b[0;31m                                   tf.math.log(tf.reshape(tf.tile(bkg_tf, [tf.shape(x)[0]]), [tf.shape(x)[0], tf.shape(bkg_tf)[0]])))), x_tf)\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;31m#print(\"type of output from map_fn is\", type(filt_list)) ##type of output from map_fn is <class 'tensorflow.python.framework.ops.Tensor'>   shape of output from map_fn is (10, 12, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#print(\"shape of output from map_fn is\", filt_list.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2711\u001b[0m       loop_var_structure = nest.map_structure(type_spec.type_spec_from_value,\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[0;32m-> 2713\u001b[0;31m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2714\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2702\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m-> 2704\u001b[0;31m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0m\u001b[1;32m   2705\u001b[0m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, _)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     _, r_a = control_flow_ops.while_loop(\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccs_ta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/museam_env/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mless\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5355\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   5356\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Less\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5357\u001b[0;31m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[1;32m   5358\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5359\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "nn_model('./sequences.fa', \n",
    "        './wt_readout.dat', \n",
    "        filters=parameters_dict[\"filters\"], \n",
    "        kernel_size=parameters_dict[\"kernel_size\"], \n",
    "        pool_type=parameters_dict[\"pool_type\"], \n",
    "        regularizer=parameters_dict[\"regularizer\"],\n",
    "        activation_type=parameters_dict[\"activation_type\"], \n",
    "        epochs=parameters_dict[\"epochs\"], \n",
    "        batch_size=parameters_dict[\"batch_size\"], \n",
    "        loss_func=parameters_dict[\"loss_func\"], \n",
    "        optimizer=parameters_dict[\"optimizer\"],\n",
    "        scaling='0_1',\n",
    "        model_name='test')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
