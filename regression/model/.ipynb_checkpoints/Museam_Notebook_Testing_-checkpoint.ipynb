{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "harmful-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# IMPORT LIBRARIES\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../preprocessing/')\n",
    "from data_preprocess import preprocess\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Tensorflow objects\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D, BatchNormalization, Activation, concatenate, ReLU\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "#from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow import keras\n",
    "from numpy import newaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defined-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suburban-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "three-audio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "#Reproducibility\n",
    "seed = 460\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "small-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET TRAIN\n",
    "# Get dictionary from text file\n",
    "def train(file_name):\n",
    "    dict = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           dict[key] = val\n",
    "\n",
    "    # change string values to integer values\n",
    "    dict[\"filters\"] = int(dict[\"filters\"])\n",
    "    dict[\"kernel_size\"] = int(dict[\"kernel_size\"])\n",
    "    dict[\"epochs\"] = int(dict[\"epochs\"])\n",
    "    dict[\"batch_size\"] = int(dict[\"batch_size\"])\n",
    "    dict[\"validation_split\"] = float(dict[\"validation_split\"])    \n",
    "    return dict\n",
    "\n",
    "def run_model(argv = None):\n",
    "    if argv is None:\n",
    "        argv = sys.argv\n",
    "        fasta_file = argv[1]\n",
    "        readout = argv[2]\n",
    "        parameter_file = argv[3]\n",
    "    else:\n",
    "        fasta_file = argv[0]\n",
    "        readout = argv[1]\n",
    "        parameter_file = argv[2]\n",
    "\n",
    "    ## excute the code\n",
    "    start_time = time.time()\n",
    "\n",
    "    parameters = train(parameter_file)\n",
    "\n",
    "    cros_eval(parameters,fasta_file,readout)\n",
    "\n",
    "    # reports time consumed during execution (secs)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET UTILS METRICS\n",
    "@tf.function()\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    #print(f'[INFO] SS_tot = {SS_tot}')\n",
    "    #print(f'[INFO] SS_res = {SS_res}')\n",
    "    coeff_det = (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "    #print(f'[INFO] Coeff_det = {coeff_det}')\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "@tf.function()    \n",
    "def spearman_fn(y_true, y_pred):\n",
    "    spearman = tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32),tf.cast(y_true, tf.float32)], Tout=tf.float32)\n",
    "    #print(f'[INFO] Spearman = {spearman}')\n",
    "    return spearman\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET CUSTOM LOSSES\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        \n",
    "@tf.function()\n",
    "def rank_mse(yTrue, yPred):\n",
    "\n",
    "  def calculate_loss(yTrue, yPred):\n",
    "    \n",
    "\n",
    "    print(f'[INFO] Print yTrue: {yTrue}')\n",
    "    print(f'[INFO] Print yPred: {yPred}')\n",
    "    \n",
    "    yTrue = tf.reshape(yTrue,shape=(1,yTrue.shape[0]))\n",
    "    yPred = tf.reshape(yPred,shape=(1,yPred.shape[0]))\n",
    "    \n",
    "    #do\n",
    "    lambda_value=1\n",
    "    size = yTrue.get_shape()[1]\n",
    "    #pass lambda value as tensor\n",
    "    lambda_value = tf.convert_to_tensor(lambda_value,dtype=\"float32\")\n",
    "    #get vector ranks\n",
    "    rank_yTrue = tf.argsort(tf.argsort(yTrue))\n",
    "    rank_yPred = tf.argsort(tf.argsort(yPred))\n",
    "    print(f'[INFO] Print ranked yTrue: {rank_yTrue}')\n",
    "    print(f'[INFO] Print ranked yPred: {rank_yPred}')\n",
    "    #calculate losses\n",
    "\n",
    "    #calculate mse\n",
    "    print(f'\\n[INFO] Calculating normal mse')\n",
    "    mse = tf.subtract(yTrue,yPred)\n",
    "    print(f'[INFO] subtract mse: {mse}')\n",
    "    mse = tf.square(mse)\n",
    "    print(f'[INFO] square mse: {mse}')\n",
    "    mse = tf.math.reduce_sum(mse).numpy()\n",
    "    print(f'[INFO] reduce sum mse: {mse}')\n",
    "    mse = tf.divide(mse,size)\n",
    "    print(f'[INFO] divide by size mse: {mse}')   \n",
    "    mse = tf.cast(mse,dtype=\"float32\")\n",
    "    print(f'[INFO] final mse: {mse}')\n",
    "  \n",
    "    #calculate rank_mse\n",
    "    print(f'\\n[INFO] Calculating rank mse')\n",
    "    rank_mse = tf.cast(tf.subtract(rank_yTrue,rank_yPred),dtype=\"float32\")\n",
    "    print(f'[INFO] substract rank_mse: {rank_mse}')\n",
    "    rank_mse = tf.square(rank_mse)\n",
    "    print(f'[INFO] square rank_mse: {rank_mse}')\n",
    "    rank_mse = tf.math.reduce_sum(rank_mse).numpy()\n",
    "    print(f'[INFO] reduce sum rank_mse: {rank_mse}')\n",
    "    #rank_mse = tf.math.sqrt(rank_mse)\n",
    "    print(f'[INFO] square root rank_mse: {rank_mse}')  \n",
    "    rank_mse = tf.divide(rank_mse,size)\n",
    "    print(f'[INFO] divide by size rank_mse: {rank_mse}') \n",
    "    print(f'[INFO] final rank_mse: {rank_mse}')\n",
    "\n",
    "    #(1 - lambda value)* mse(part a of loss)\n",
    "    loss_a = tf.multiply(tf.subtract(tf.ones(1,dtype=\"float32\"),lambda_value),mse)\n",
    "    print(f'\\n[INFO] Final loss a: {loss_a}')\n",
    "    #lambda value * rank_mse (part b of loss)\n",
    "    loss_b = tf.multiply(lambda_value,rank_mse)\n",
    "    print(f'[INFO] Final loss b: {loss_b}')\n",
    "    #final loss\n",
    "    loss = tf.add(loss_a,loss_b)\n",
    "    print(f'[INFO] Final loss: {loss}')\n",
    "    return loss\n",
    "\n",
    "  debug=False\n",
    "\n",
    "  if not debug:\n",
    "    with HiddenPrints():\n",
    "      loss = calculate_loss(yTrue, yPred)\n",
    "      return loss\n",
    "  else:\n",
    "    loss = calculate_loss(yTrue, yPred)\n",
    "    return loss\n",
    "\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# SET MODEL CONSTRUCTION\n",
    "class ConvolutionLayer(Conv1D):\n",
    "    def __init__(self, \n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 data_format,\n",
    "                 padding='valid',\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 __name__ = 'ConvolutionLayer',\n",
    "                 **kwargs):\n",
    "                 \n",
    "        super(ConvolutionLayer, self).__init__(filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            **kwargs)\n",
    "        self.run_value = 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "      ## shape of self.kernel is (12, 4, 512)\n",
    "      ##the type of self.kernel is <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "        if self.run_value > 2:\n",
    "\n",
    "            x_tf = self.kernel  ##x_tf after reshaping is a tensor and not a weight variable :(\n",
    "            x_tf = tf.transpose(x_tf, [2, 0, 1])\n",
    "\n",
    "            alpha = 100\n",
    "            beta = 1/alpha\n",
    "            bkg = tf.constant([0.295, 0.205, 0.205, 0.295])\n",
    "            bkg_tf = tf.cast(bkg, tf.float32)\n",
    "            filt_list = tf.map_fn(lambda x:\n",
    "                                  tf.math.scalar_mul(beta, tf.subtract(tf.subtract(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1)),\n",
    "                                  tf.expand_dims(tf.math.log(tf.math.reduce_sum(tf.math.exp(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1))), axis = 1)), axis = 1)),\n",
    "                                  tf.math.log(tf.reshape(tf.tile(bkg_tf, [tf.shape(x)[0]]), [tf.shape(x)[0], tf.shape(bkg_tf)[0]])))), x_tf)\n",
    "            #print(\"type of output from map_fn is\", type(filt_list)) ##type of output from map_fn is <class 'tensorflow.python.framework.ops.Tensor'>   shape of output from map_fn is (10, 12, 4)\n",
    "            #print(\"shape of output from map_fn is\", filt_list.shape)\n",
    "            #transf = tf.reshape(filt_list, [12, 4, self.filters]) ##12, 4, 512\n",
    "            transf = tf.transpose(filt_list, [1, 2, 0])\n",
    "            ##type of transf is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "            outputs = self._convolution_op(inputs, transf) ## type of outputs is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "\n",
    "        else:\n",
    "            outputs = self._convolution_op(inputs, self.kernel)\n",
    "        self.run_value += 1\n",
    "        return outputs\n",
    "\n",
    "class Museam:\n",
    "    def __init__(self,\n",
    "                 dim_num,\n",
    "                 filters, \n",
    "                 kernel_size, \n",
    "                 pool_type, \n",
    "                 regularizer, \n",
    "                 activation_type, \n",
    "                 epochs,\n",
    "                 batch_size, \n",
    "                 loss_func, \n",
    "                 optimizer,\n",
    "                 model_name):\n",
    "\n",
    "        \"\"\"initialize basic parameters\"\"\"\n",
    "        self.dim_num = dim_num\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_type = pool_type\n",
    "        self.regularizer = regularizer\n",
    "        self.activation_type = activation_type\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        dim_num = self.dim_num\n",
    "\n",
    "        # Input Node\n",
    "        forward = tf.keras.Input(shape=(dim_num[1],dim_num[2]), name = 'forward')\n",
    "        reverse = tf.keras.Input(shape=(dim_num[1],dim_num[2]), name = 'reverse')\n",
    "\n",
    "        # Multinomial Layer\n",
    "        first_layer = ConvolutionLayer(filters=self.filters, \n",
    "                                       kernel_size=self.kernel_size, \n",
    "                                       strides=1, \n",
    "                                       data_format='channels_last', \n",
    "                                       use_bias = True)\n",
    "\n",
    "        fw = first_layer(forward)\n",
    "        bw = first_layer(reverse)\n",
    "\n",
    "        # Concatenate both strands\n",
    "        concat = concatenate([fw, bw], axis=1)\n",
    "        pool_size_input = concat.shape[1]\n",
    "        concat_relu = ReLU()(concat)\n",
    "\n",
    "        #Pooling Layer\n",
    "        if self.pool_type == 'Max':\n",
    "            pool_layer = MaxPooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "            #pool_layer = MaxPooling1D(pool_size=12)(concat_relu)\n",
    "        elif self.pool_type == 'Ave':\n",
    "            pool_layer = AveragePooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "        elif self.pool_type == 'custom':\n",
    "            def out_shape(input_shape):\n",
    "                shape = list(input_shape)\n",
    "                print(input_shape)\n",
    "                shape[0] = 10\n",
    "                return tuple(shape)\n",
    "            #model.add(Lambda(top_k, arguments={'k': 10}))\n",
    "            def top_k(inputs, k):\n",
    "                # tf.nn.top_k Finds values and indices of the k largest entries for the last dimension\n",
    "                print(inputs.shape)\n",
    "                inputs2 = tf.transpose(inputs, [0,2,1])\n",
    "                new_vals = tf.nn.top_k(inputs2, k=k, sorted=True).values\n",
    "                # transform back to (None, 10, 512)\n",
    "                return tf.transpose(new_vals, [0,2,1])\n",
    "\n",
    "            pool_layer = Lambda(top_k, arguments={'k': 2})(concat_relu)\n",
    "            pool_layer = AveragePooling1D(pool_size=2)(pool_layer)\n",
    "        elif self.pool_type == 'custom_sum':\n",
    "            ## apply relu function before custom_sum functions\n",
    "            def summed_up(inputs):\n",
    "                #nonzero_vals = tf.keras.backend.relu(inputs)\n",
    "                new_vals = tf.math.reduce_sum(inputs, axis = 1, keepdims = True)\n",
    "                return new_vals\n",
    "            pool_layer = Lambda(summed_up)(concat_relu)\n",
    "        else:\n",
    "            raise NameError('Set the pooling layer name correctly')\n",
    "\n",
    "        # Flatten Layer (None, 512)\n",
    "        flat = Flatten()(pool_layer)\n",
    "        if self.activation_type == 'linear':\n",
    "            if self.regularizer == 'L_1':\n",
    "                outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l1(0.001), activation= self.activation_type)(flat)\n",
    "            elif self.regularizer == 'L_2':\n",
    "                outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation= self.activation_type)(flat)\n",
    "            else:\n",
    "                raise NameError('Set the regularizer name correctly')\n",
    "        elif self.activation_type =='sigmoid':\n",
    "            outputs = Dense(1, activation= self.activation_type)(flat)\n",
    "\n",
    "        # Model Creation\n",
    "        model = keras.Model(inputs=[forward, reverse], outputs=outputs)\n",
    "\n",
    "        ## Model Summary\n",
    "        #model.summary()\n",
    "        #if self.loss_func == 'mse':\n",
    "        #    model.compile(loss='mean_squared_error', optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'huber':\n",
    "        #    loss_huber = keras.losses.Huber(delta=1)\n",
    "        #    model.compile(loss=loss_huber, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'mae':\n",
    "        #    loss_mae = keras.losses.MeanAbsoluteError()\n",
    "        #    model.compile(loss=loss_mae, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'rank_mse':\n",
    "        #    model.compile(loss=rank_mse, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'poisson':\n",
    "        #    poisson_loss = keras.losses.Poisson()\n",
    "        #    model.compile(loss=poisson_loss, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        #elif self.loss_func == 'binary_crossentropy':\n",
    "        #    binary_crossentropy_loss = keras.losses.BinaryCrossentropy()\n",
    "        #    model.compile(loss=binary_crossentropy_loss, optimizer=self.optimizer, metrics = ['binary_accuracy'])\n",
    "        #else:\n",
    "        #    raise NameError('Unrecognized Loss Function')\n",
    "\n",
    "        return model\n",
    "\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# EVAL MODEL\n",
    "\n",
    "def cros_eval(parameters,\n",
    "              fasta_file_positive,\n",
    "              readout):\n",
    "\n",
    "    # Preprocess the data\n",
    "    prep = preprocess(f'../data/{fasta_file_positive}',f'../data/{readout}')\n",
    "    names = prep.read_fasta_name_into_array()\n",
    "    prep_dict = prep.one_hot_encode()\n",
    "    \n",
    "    \n",
    "    fw_fasta = prep_dict[\"forward\"]\n",
    "    rc_fasta = prep_dict[\"reverse\"]\n",
    "    readout =  prep_dict[\"readout\"]\n",
    "    \n",
    "    if parameters['activation_type'] == 'linear':\n",
    "        readout = np.log2(readout)\n",
    "        if parameters['scaling'] == None:\n",
    "            readout = np.ndarray.tolist(readout)\n",
    "        elif parameters['scaling'] == \"0_1\":\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaler.fit(readout.reshape(-1, 1))\n",
    "            readout = scaler.transform(readout.reshape(-1, 1))\n",
    "            readout = readout.flatten()\n",
    "            readout = np.ndarray.tolist(readout)\n",
    "      \n",
    "    # Shuffle the data\n",
    "    forward_shuffle, readout_shuffle, names_shuffle = shuffle(fw_fasta, readout, names, random_state=seed)\n",
    "    reverse_shuffle, readout_shuffle, names_shuffle = shuffle(rc_fasta, readout, names, random_state=seed)\n",
    "    readout_shuffle = np.array(readout_shuffle)\n",
    "    \n",
    "    # Get dim\n",
    "    dim_num = forward_shuffle.shape\n",
    "\n",
    "\n",
    "    #initialize metrics to save values \n",
    "    metrics = []\n",
    "\n",
    "    #Provides train/test indices to split data in train/test sets.\n",
    "    kFold = StratifiedKFold(n_splits=10)\n",
    "    ln = np.zeros(len(readout_shuffle))\n",
    "        \n",
    "    pred_vals = pandas.DataFrame()\n",
    "    cv_results =pandas.DataFrame()\n",
    "\n",
    "    Fold=0\n",
    "    model_name = parameters['model_name']\n",
    "\n",
    "    for train, test in kFold.split(ln, ln):\n",
    "        print(f'[INFO] CALCULATING FOLD=={Fold}')\n",
    "\n",
    "        model = Museam(dim_num,\n",
    "                      parameters['filters'], \n",
    "                      parameters['kernel_size'], \n",
    "                      parameters['pool_type'], \n",
    "                      parameters['regularizer'], \n",
    "                      parameters['activation_type'], \n",
    "                      parameters['epochs'],\n",
    "                      parameters['batch_size'], \n",
    "                      parameters['loss_func'], \n",
    "                      parameters['optimizer'],\n",
    "                      parameters['model_name']).create_model()\n",
    "  \n",
    "\n",
    "        # Get splits\n",
    "        fwd_train = forward_shuffle[train]\n",
    "        rc_train = reverse_shuffle[train]\n",
    "        fwd_test = forward_shuffle[test]\n",
    "        rc_test = reverse_shuffle[test]\n",
    "        y_train = readout_shuffle[train]\n",
    "        y_test = readout_shuffle[test]\n",
    "        names_train = names_shuffle[test]\n",
    "        names_test = names_shuffle[test]\n",
    "        \n",
    "        print(fwd_train.shape)\n",
    "        \n",
    "\n",
    "        # Train model\n",
    "        \n",
    "        model.compile(loss='mean_squared_error', \n",
    "                      optimizer=parameters['optimizer'], \n",
    "                      metrics = [coeff_determination, spearman_fn],\n",
    "                      run_eagerly=False)\n",
    "        \n",
    "        model.fit({'forward': fwd_train, 'reverse': rc_train}, \n",
    "                            y_train, \n",
    "                            epochs=20, \n",
    "                            batch_size=parameters['batch_size'], \n",
    "                            validation_split=parameters['validation_split'],\n",
    "                            workers=4\n",
    "                            )\n",
    "        \n",
    "        model.compile(loss=rank_mse, \n",
    "                      optimizer=parameters['optimizer'],\n",
    "                      metrics = [coeff_determination, spearman_fn],\n",
    "                      run_eagerly=True\n",
    "                     )\n",
    "        \n",
    "        model.fit({'forward': fwd_train, 'reverse': rc_train}, \n",
    "                            y_train, \n",
    "                            epochs=5, \n",
    "                            batch_size=2196, \n",
    "                            validation_split=parameters['validation_split']\n",
    "                            )\n",
    "\n",
    "        # Get metrics\n",
    "        loss, R2, Spearman = model.evaluate({'forward': fwd_test, 'reverse': rc_test}, y_test)\n",
    "        pred = model.predict({'forward': fwd_test, 'reverse': rc_test})\n",
    "        pred = np.reshape(pred,len(pred))\n",
    "        \n",
    "\n",
    "        # Temporary fold dataframes\n",
    "        temp = pandas.DataFrame({'sequence_names':np.array(names_test).flatten(),\n",
    "                                     'true_vals':np.array(y_test).flatten(),\n",
    "                                     'pred_vals':np.array(pred).flatten()})                        \n",
    "        temp['Fold'] = Fold\n",
    "\n",
    "        temp2 = pandas.DataFrame({\"Fold\":[Fold],\n",
    "                                  \"R2\":[R2],\n",
    "                                  \"Spearman\":[Spearman],\n",
    "                                  \"Loss\":[loss], \n",
    "        })\n",
    "\n",
    "        Fold=Fold+1\n",
    "        \n",
    "\n",
    "        #append to main dataframe\n",
    "        pred_vals = pred_vals.append(temp,ignore_index=True)\n",
    "        cv_results = cv_results.append(temp2, ignore_index=True)\n",
    "\n",
    "    \n",
    "    pred_vals.to_csv(f'../outs/metrics/{model_name}.csv')\n",
    "\n",
    "\n",
    "    #calculate mean accuracy across all folds\n",
    "    mean_R2 = cv_results['R2'].mean()\n",
    "    mean_spearman = cv_results['Spearman'].mean()\n",
    "    cv_results = cv_results.append({'Fold':'All folds','Loss':'None','Spearman':mean_spearman,'R2':mean_R2}, ignore_index=True)\n",
    "    cv_results.to_csv(f'../outs/metrics/{model_name}_cv_results.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "greek-orleans",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CALCULATING FOLD==0\n",
      "(2196, 171, 4)\n",
      "Epoch 1/20\n",
      "13/38 [=========>....................] - ETA: 47s - loss: 0.0527 - coeff_determination: -1.1384 - spearman_fn: 0.1528"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e5e98c94f6f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sequences.fa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wt_readout.dat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../parameters/parameters_mixed_mse_and_rank_mse.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c3c94769b81c>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcros_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfasta_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreadout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# reports time consumed during execution (secs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c3c94769b81c>\u001b[0m in \u001b[0;36mcros_eval\u001b[0;34m(parameters, fasta_file_positive, readout)\u001b[0m\n\u001b[1;32m    409\u001b[0m                       run_eagerly=False)\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         model.fit({'forward': fwd_train, 'reverse': rc_train}, \n\u001b[0m\u001b[1;32m    412\u001b[0m                             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m       outputs = self.distribute_strategy.run(\n\u001b[0m\u001b[1;32m    572\u001b[0m           self.train_step, args=(data,))\n\u001b[1;32m    573\u001b[0m       outputs = reduce_per_replica(\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    949\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m    950\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m--> 951\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m   \u001b[0;31m# TODO(b/151224785): Remove deprecated alias.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2288\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2649\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;31m# The _minimize call does a few extra steps unnecessary in most cases,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;31m# such as loss scaling and gradient clipping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n\u001b[0m\u001b[1;32m    541\u001b[0m               self.trainable_variables)\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_minimize\u001b[0;34m(strategy, tape, optimizer, loss, trainable_variables)\u001b[0m\n\u001b[1;32m   1787\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scaled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m   \u001b[0;31m# Whether to aggregate gradients outside of optimizer. This requires support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1043\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MaxGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_MaxGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;34m\"\"\"Gradient for Max.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_MinOrMaxGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MinOrMaxGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0mindicators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m   num_selected = array_ops.reshape(\n\u001b[0;32m--> 233\u001b[0;31m       math_ops.reduce_sum(indicators, op.inputs[1]), output_shape_kept_dims)\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindicators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_selected\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   1739\u001b[0m   \"\"\"\n\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   1742\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   1751\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   1752\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/STANN/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  10153\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10154\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10155\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  10156\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10157\u001b[0m         input, axis, \"keep_dims\", keep_dims)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "# RUN SCRIPT\n",
    "\n",
    "#run_model()\n",
    "#nohup python deepsea.py sequences.fa wt_readout.dat parameters. > outs/deepsea.out &\n",
    "\n",
    "\n",
    "#run locally\n",
    "\n",
    "argv = ['sequences.fa','wt_readout.dat','../parameters/parameters_mixed_mse_and_rank_mse.txt']\n",
    "run_model(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-builder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('STANN': conda)",
   "language": "python",
   "name": "python38564bitstanncondab5b9ed665b0247feb8508c2909641d6e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
