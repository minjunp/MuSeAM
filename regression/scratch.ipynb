{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('museam_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e1b2108af56e0534cb60fc6f36fc8b9d20f5606f6a579c5ffad689c3e959bd13"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/franciscogrisanti/anaconda3/envs/museam_env/bin/python'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "import keras\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D, BatchNormalization, Activation, concatenate, ReLU\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_preprocess import preprocess\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from numpy import newaxis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#Reproducibility\n",
    "seed = 460\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Create new loss function (Rank mse)\n",
    "@tf.function()\n",
    "def rank_mse(yTrue, yPred):\n",
    "  lambda_value=0.25\n",
    "  #pass lambda value as tensor\n",
    "  lambda_value = tf.convert_to_tensor(lambda_value,dtype=\"float32\")\n",
    "\n",
    "  #get vector ranks\n",
    "  rank_yTrue = tf.argsort(tf.argsort(yTrue))\n",
    "  rank_yPred = tf.argsort(tf.argsort(yPred))\n",
    "\n",
    "  #calculate losses\n",
    "  mse = tf.reduce_mean(tf.square(tf.subtract(yTrue,yPred)))\n",
    "  rank_mse = tf.reduce_mean(tf.square(tf.subtract(rank_yTrue,rank_yPred)))\n",
    "\n",
    "  #take everything to same dtype\n",
    "  mse = tf.cast(mse,dtype=\"float32\")\n",
    "  rank_mse = tf.cast(rank_mse,dtype=\"float32\")\n",
    "\n",
    "  #(1 - lambda value)* mse(part a of loss)\n",
    "  loss_a = tf.multiply(tf.subtract(tf.ones(1,dtype=\"float32\"),lambda_value),mse)\n",
    "  #lambda value * rank_mse (part b of loss)\n",
    "  loss_b = tf.multiply(lambda_value,rank_mse)\n",
    "  #final loss\n",
    "  loss = tf.add(loss_a,loss_b)\n",
    "\n",
    "  return loss\n",
    "\n",
    "class ConvolutionLayer(Conv1D):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 data_format,\n",
    "                 padding='valid',\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 __name__ = 'ConvolutionLayer',\n",
    "                 **kwargs):\n",
    "        super(ConvolutionLayer, self).__init__(filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            **kwargs)\n",
    "        self.run_value = 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "      ## shape of self.kernel is (12, 4, 512)\n",
    "      ##the type of self.kernel is <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
    "        if self.run_value > 2:\n",
    "\n",
    "            x_tf = self.kernel  ##x_tf after reshaping is a tensor and not a weight variable :(\n",
    "            x_tf = tf.transpose(x_tf, [2, 0, 1])\n",
    "\n",
    "            alpha = 100\n",
    "            beta = 1/alpha\n",
    "            bkg = tf.constant([0.295, 0.205, 0.205, 0.295])\n",
    "            bkg_tf = tf.cast(bkg, tf.float32)\n",
    "            filt_list = tf.map_fn(lambda x:\n",
    "                                  tf.math.scalar_mul(beta, tf.subtract(tf.subtract(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1)),\n",
    "                                  tf.expand_dims(tf.math.log(tf.math.reduce_sum(tf.math.exp(tf.subtract(tf.math.scalar_mul(alpha, x),\n",
    "                                  tf.expand_dims(tf.math.reduce_max(tf.math.scalar_mul(alpha, x), axis = 1), axis = 1))), axis = 1)), axis = 1)),\n",
    "                                  tf.math.log(tf.reshape(tf.tile(bkg_tf, [tf.shape(x)[0]]), [tf.shape(x)[0], tf.shape(bkg_tf)[0]])))), x_tf)\n",
    "            #print(\"type of output from map_fn is\", type(filt_list)) ##type of output from map_fn is <class 'tensorflow.python.framework.ops.Tensor'>   shape of output from map_fn is (10, 12, 4)\n",
    "            #print(\"shape of output from map_fn is\", filt_list.shape)\n",
    "            #transf = tf.reshape(filt_list, [12, 4, self.filters]) ##12, 4, 512\n",
    "            transf = tf.transpose(filt_list, [1, 2, 0])\n",
    "            ##type of transf is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "            outputs = self._convolution_op(inputs, transf) ## type of outputs is <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "\n",
    "        else:\n",
    "            outputs = self._convolution_op(inputs, self.kernel)\n",
    "        self.run_value += 1\n",
    "        return outputs\n",
    "\n",
    "class nn_model:\n",
    "    def __init__(self, fasta_file, readout_file, filters, kernel_size, pool_type, regularizer, activation_type, epochs, batch_size, loss_func, optimizer):\n",
    "        \"\"\"initialize basic parameters\"\"\"\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_type = pool_type\n",
    "        self.regularizer = regularizer\n",
    "        self.activation_type = activation_type\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.fasta_file = fasta_file\n",
    "        self.readout_file = readout_file\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        #self.eval()\n",
    "        self.cross_val()\n",
    "        #self.cross_val_binning()\n",
    "\n",
    "    def create_model(self):\n",
    "        # different metric functions\n",
    "        def coeff_determination(y_true, y_pred):\n",
    "            SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "            SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "            return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "        def spearman_fn(y_true, y_pred):\n",
    "            return tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32),\n",
    "                   tf.cast(y_true, tf.float32)], Tout=tf.float32)\n",
    "\n",
    "        # building model\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "        # if want mono-nucleotide sequences\n",
    "        dict = prep.one_hot_encode()\n",
    "        # if want dinucleotide sequences\n",
    "        #dict = prep.dinucleotide_encode()\n",
    "\n",
    "        readout = dict[\"readout\"]\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "\n",
    "        dim_num = fw_fasta.shape\n",
    "\n",
    "        # To build this model with the functional API,\n",
    "        # you would start by creating an input node:\n",
    "        forward = keras.Input(shape=(dim_num[1],dim_num[2]), name = 'forward')\n",
    "        reverse = keras.Input(shape=(dim_num[1],dim_num[2]), name = 'reverse')\n",
    "\n",
    "        #first_layer = Conv1D(filters=self.filters, kernel_size=self.kernel_size, data_format='channels_last', input_shape=(dim_num[1],dim_num[2]), use_bias = True)\n",
    "        first_layer = ConvolutionLayer(filters=self.filters, kernel_size=self.kernel_size, strides=1, data_format='channels_last', use_bias = True)\n",
    "\n",
    "        fw = first_layer(forward)\n",
    "        bw = first_layer(reverse)\n",
    "\n",
    "        concat = concatenate([fw, bw], axis=1)\n",
    "        pool_size_input = concat.shape[1]\n",
    "\n",
    "        concat_relu = ReLU()(concat)\n",
    "\n",
    "        if self.pool_type == 'Max':\n",
    "            pool_layer = MaxPooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "            #pool_layer = MaxPooling1D(pool_size=12)(concat_relu)\n",
    "        elif self.pool_type == 'Ave':\n",
    "            pool_layer = AveragePooling1D(pool_size=pool_size_input)(concat_relu)\n",
    "        elif self.pool_type == 'custom':\n",
    "            def out_shape(input_shape):\n",
    "                shape = list(input_shape)\n",
    "                print(input_shape)\n",
    "                shape[0] = 10\n",
    "                return tuple(shape)\n",
    "            #model.add(Lambda(top_k, arguments={'k': 10}))\n",
    "            def top_k(inputs, k):\n",
    "                # tf.nn.top_k Finds values and indices of the k largest entries for the last dimension\n",
    "                print(inputs.shape)\n",
    "                inputs2 = tf.transpose(inputs, [0,2,1])\n",
    "                new_vals = tf.nn.top_k(inputs2, k=k, sorted=True).values\n",
    "                # transform back to (None, 10, 512)\n",
    "                return tf.transpose(new_vals, [0,2,1])\n",
    "\n",
    "            pool_layer = Lambda(top_k, arguments={'k': 2})(concat_relu)\n",
    "            pool_layer = AveragePooling1D(pool_size=2)(pool_layer)\n",
    "        elif self.pool_type == 'custom_sum':\n",
    "            ## apply relu function before custom_sum functions\n",
    "            def summed_up(inputs):\n",
    "                #nonzero_vals = tf.keras.backend.relu(inputs)\n",
    "                new_vals = tf.math.reduce_sum(inputs, axis = 1, keepdims = True)\n",
    "                return new_vals\n",
    "            pool_layer = Lambda(summed_up)(concat_relu)\n",
    "\n",
    "        else:\n",
    "            raise NameError('Set the pooling layer name correctly')\n",
    "\n",
    "        #layer = Conv1D(filters=128, kernel_size=12)(pool_layer)\n",
    "        #layer = Dense(16)(pool_layer)\n",
    "        #pool_size_input = layer.shape[1]\n",
    "        #layer = MaxPooling1D(pool_size=pool_size_input)(layer)\n",
    "\n",
    "        # flatten the layer (None, 512)\n",
    "        flat = Flatten()(pool_layer)\n",
    "\n",
    "        if self.regularizer == 'L_1':\n",
    "            outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l1(0.001), activation= self.activation_type)(flat)\n",
    "        elif self.regularizer == 'L_2':\n",
    "            outputs = Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation= self.activation_type)(flat)\n",
    "        else:\n",
    "            raise NameError('Set the regularizer name correctly')\n",
    "\n",
    "        model = keras.Model(inputs=[forward, reverse], outputs=outputs)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        if self.loss_func == 'mse':\n",
    "            model.compile(loss='mean_squared_error', optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        elif self.loss_func == 'huber':\n",
    "            loss_huber = keras.losses.Huber(delta=1)\n",
    "            model.compile(loss=loss_huber, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        elif self.loss_func == 'mae':\n",
    "            loss_mae = keras.losses.MeanAbsoluteError()\n",
    "            model.compile(loss=loss_mae, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        elif self.loss_func == 'rank_mse':\n",
    "            model.compile(loss=rank_mse, optimizer=self.optimizer, metrics = [coeff_determination, spearman_fn])\n",
    "        else:\n",
    "            raise NameError('Unrecognized Loss Function')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def eval(self):\n",
    "\n",
    "        # Preprocess the data to one-hot encoded vector\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "\n",
    "        dict = prep.one_hot_encode()\n",
    "\n",
    "        # if want dinucleotide sequences\n",
    "        # dict = prep.dinucleotide_encode()\n",
    "\n",
    "        # print maximum length without truncation\n",
    "        np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "        readout = dict[\"readout\"]\n",
    "\n",
    "        if self.activation_type == 'linear':\n",
    "            readout = np.log2(readout)\n",
    "            readout = np.ndarray.tolist(readout)\n",
    "\n",
    "        # 90% Train, 10% Test\n",
    "        x1_train, x1_test, y1_train, y1_test = train_test_split(fw_fasta, readout, test_size=0.1, random_state=seed)\n",
    "        x2_train, x2_test, y2_train, y2_test = train_test_split(rc_fasta, readout, test_size=0.1, random_state=seed)\n",
    "\n",
    "        model = self.create_model()\n",
    "\n",
    "        # change from list to numpy array\n",
    "        y1_train = np.asarray(y1_train)\n",
    "        y1_test = np.asarray(y1_test)\n",
    "        y2_train = np.asarray(y2_train)\n",
    "        y2_test = np.asarray(y2_test)\n",
    "\n",
    "\n",
    "        # Without early stopping\n",
    "        #history = model.fit({'forward': x1_train, 'reverse': x2_train}, y1_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.1)\n",
    "\n",
    "        # Early stopping\n",
    "        #callback = EarlyStopping(monitor='loss', min_delta=0.001, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "        callback = EarlyStopping(monitor='val_spearman_fn', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "        history = model.fit({'forward': x1_train, 'reverse': x2_train}, y1_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.1, callbacks = [callback])\n",
    "\n",
    "        history2 = model.evaluate({'forward': x1_test, 'reverse': x2_test}, y1_test)\n",
    "        pred = model.predict({'forward': x1_test, 'reverse': x2_test})\n",
    "\n",
    "        #viz_prediction(pred, y1_test, '{} regression model'.format(self.loss_func), '{}2.png'.format(self.loss_func))\n",
    "\n",
    "        print(\"Seed number is {}\".format(seed))\n",
    "        print('metric values of model.evaluate: '+ str(history2))\n",
    "        print('metrics names are ' + str(model.metrics_names))\n",
    "\n",
    "    def cross_val(self):\n",
    "        # Preprocess the data\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "        dict = prep.one_hot_encode()\n",
    "        # If want dinucleotide sequences\n",
    "        #dict = prep.dinucleotide_encode()\n",
    "\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "        readout = dict[\"readout\"]\n",
    "\n",
    "        if self.activation_type == 'linear':\n",
    "            readout = np.log2(readout)\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaler.fit(readout.reshape(-1, 1))\n",
    "            readout = scaler.transform(readout.reshape(-1, 1))\n",
    "            readout = readout.flatten()\n",
    "            readout = np.ndarray.tolist(readout)   \n",
    "\n",
    "\n",
    "\n",
    "        forward_shuffle, readout_shuffle = shuffle(fw_fasta, readout, random_state=seed)\n",
    "        reverse_shuffle, readout_shuffle = shuffle(rc_fasta, readout, random_state=seed)\n",
    "        readout_shuffle = np.array(readout_shuffle)\n",
    "\n",
    "        # initialize metrics to save values\n",
    "        metrics = []\n",
    "\n",
    "        # Provides train/test indices to split data in train/test sets.\n",
    "        kFold = StratifiedKFold(n_splits=10)\n",
    "        ln = np.zeros(len(readout_shuffle))\n",
    "        true_vals = []\n",
    "        pred_vals = []\n",
    "\n",
    "        for train, test in kFold.split(ln, ln):\n",
    "            model = None\n",
    "            model = self.create_model()\n",
    "\n",
    "            fwd_train = forward_shuffle[train]\n",
    "            fwd_test = forward_shuffle[test]\n",
    "            rc_train = reverse_shuffle[train]\n",
    "            rc_test = reverse_shuffle[test]\n",
    "            y_train = readout_shuffle[train]\n",
    "            y_test = readout_shuffle[test]\n",
    "\n",
    "            # Early stopping\n",
    "            #callback = EarlyStopping(monitor='loss', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #callback = EarlyStopping(monitor='val_spearman_fn', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0, callbacks = [callback])\n",
    "\n",
    "            history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0)\n",
    "            history2 = model.evaluate({'forward': fwd_test, 'reverse': rc_test}, y_test)\n",
    "            pred = model.predict({'forward': fwd_test, 'reverse': rc_test})\n",
    "\n",
    "            metrics.append(history2)\n",
    "            pred = np.reshape(pred,len(pred))\n",
    "            true_vals.append(y_test.tolist())\n",
    "            pred_vals.append(pred.tolist())\n",
    "\n",
    "        g1 = []\n",
    "        g2 = []\n",
    "        g3 = []\n",
    "        for i in metrics:\n",
    "            loss, r_2, spearman_val = i\n",
    "            g1.append(loss)\n",
    "            g2.append(r_2)\n",
    "            g3.append(spearman_val)\n",
    "\n",
    "        #np.savetxt('true_vals.txt', true_vals)\n",
    "        #np.savetxt('pred_vals.txt', pred_vals)\n",
    "        #viz_prediction(pred_vals, true_vals, '{} delta=1 regression model (seed=460)'.format(self.loss_func), '{}_d1.png'.format(self.loss_func))\n",
    "\n",
    "        print(g2)\n",
    "        print(g3)\n",
    "        print('seed number = %d' %seed)\n",
    "        print('Mean loss of 10-fold cv is ' + str(np.mean(g1)))\n",
    "        print('Mean R_2 score of 10-fold cv is ' + str(np.mean(g2)))\n",
    "        print('Mean Spearman of 10-fold cv is ' + str(np.mean(g3)))\n",
    "\n",
    "    def cross_val_binning(self):\n",
    "        # Preprocess the data\n",
    "        prep = preprocess(self.fasta_file, self.readout_file)\n",
    "        dict = prep.one_hot_encode()\n",
    "        # If want dinucleotide sequences\n",
    "        #dict = prep.dinucleotide_encode()\n",
    "\n",
    "        fw_fasta = dict[\"forward\"]\n",
    "        rc_fasta = dict[\"reverse\"]\n",
    "        readout = dict[\"readout\"]\n",
    "\n",
    "        if self.activation_type == 'linear':\n",
    "            readout = np.log2(readout)\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaler.fit(readout.reshape(-1, 1))\n",
    "            readout = scaler.transform(readout.reshape(-1, 1))\n",
    "            readout = readout.flatten()\n",
    "            readout = np.ndarray.tolist(readout)    \n",
    "\n",
    "        # Returns the indices that would sort an array.\n",
    "        x = np.argsort(readout)\n",
    "        ind = []\n",
    "\n",
    "        num = 10\n",
    "        for i in range(num):\n",
    "            for j in range(int(len(x)/num)):\n",
    "                id = i + j * num\n",
    "                ind.append(x[id])\n",
    "\n",
    "        forward_bin = fw_fasta[ind]\n",
    "        reverse_bin = rc_fasta[ind]\n",
    "        readout_bin = readout[ind]\n",
    "        #readout_bin = np.array(readout_bin)\n",
    "\n",
    "        # initialize metrics to save values\n",
    "        metrics = []\n",
    "\n",
    "        # Provides train/test indices to split data in train/test sets.\n",
    "        kFold = StratifiedKFold(n_splits=10)\n",
    "        ln = np.zeros(len(readout_bin))\n",
    "        true_vals = []\n",
    "        pred_vals = []\n",
    "\n",
    "        for train, test in kFold.split(ln, ln):\n",
    "            model = None\n",
    "            model = self.create_model()\n",
    "\n",
    "            fwd_train = forward_bin[train]\n",
    "            fwd_test = forward_bin[test]\n",
    "            rc_train = reverse_bin[train]\n",
    "            rc_test = reverse_bin[test]\n",
    "            y_train = readout_bin[train]\n",
    "            y_test = readout_bin[test]\n",
    "\n",
    "            # Early stopping\n",
    "            #callback = EarlyStopping(monitor='loss', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #callback = EarlyStopping(monitor='val_spearman_fn', min_delta=0.0001, patience=3, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "            #history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0, callbacks = [callback])\n",
    "\n",
    "            history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0)\n",
    "            history2 = model.evaluate({'forward': fwd_test, 'reverse': rc_test}, y_test)\n",
    "            pred = model.predict({'forward': fwd_test, 'reverse': rc_test})\n",
    "\n",
    "            metrics.append(history2)\n",
    "            pred = np.reshape(pred,len(pred))\n",
    "            true_vals.append(y_test.tolist())\n",
    "            pred_vals.append(pred.tolist())\n",
    "\n",
    "        g1 = []\n",
    "        g2 = []\n",
    "        g3 = []\n",
    "        for i in metrics:\n",
    "            loss, r_2, spearman_val = i\n",
    "            g1.append(loss)\n",
    "            g2.append(r_2)\n",
    "            g3.append(spearman_val)\n",
    "\n",
    "        #np.savetxt('true_vals.txt', true_vals)\n",
    "        #np.savetxt('pred_vals.txt', pred_vals)\n",
    "        #viz_prediction(pred_vals, true_vals, '{} delta=1 regression model (seed=460)'.format(self.loss_func), '{}_d1.png'.format(self.loss_func))\n",
    "\n",
    "        print(g2)\n",
    "        print(g3)\n",
    "        print('seed number = %d' %seed)\n",
    "        print('Mean loss of 10-fold cv is ' + str(np.mean(g1)))\n",
    "        print('Mean R_2 score of 10-fold cv is ' + str(np.mean(g2)))\n",
    "        print('Mean Spearman of 10-fold cv is ' + str(np.mean(g3)))\n",
    "\n",
    "def viz_prediction(pred, true, title_name, file_name):\n",
    "    # plot true vs pred\n",
    "    plt.plot(pred, true, 'o', markersize=1)\n",
    "    plt.xlabel('Predicted Enhancer Activity')\n",
    "    plt.ylabel('True Enhancer Activity')\n",
    "    plt.title(title_name)\n",
    "    plt.xlim(-0.75, 3)\n",
    "    plt.ylim(-0.75, 3)\n",
    "    plt.savefig(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(file_name):\n",
    "    dict = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "           (key, val) = line.split()\n",
    "           dict[key] = val\n",
    "\n",
    "    # change string values to integer values\n",
    "    dict[\"filters\"] = int(dict[\"filters\"])\n",
    "    dict[\"kernel_size\"] = int(dict[\"kernel_size\"])\n",
    "    dict[\"epochs\"] = int(dict[\"epochs\"])\n",
    "    dict[\"batch_size\"] = int(dict[\"batch_size\"])\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'filters': 512,\n",
       " 'kernel_size': 12,\n",
       " 'pool_type': 'Max',\n",
       " 'regularizer': 'L_1',\n",
       " 'activation_type': 'linear',\n",
       " 'epochs': 30,\n",
       " 'batch_size': 59,\n",
       " 'loss_func': 'rank_mse',\n",
       " 'optimizer': 'adam',\n",
       " 'scaling': '0_1',\n",
       " 'model_name': 'rank_mse_0_1'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "parameters_dict = train('./parameters_rank_mse.txt')\n",
    "parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocess('./sequences.fa', './wt_readout.dat')\n",
    "dict = prep.one_hot_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocess('./sequences.fa', './wt_readout.dat')\n",
    "dict = prep.one_hot_encode()\n",
    "# If want dinucleotide sequences\n",
    "#dict = prep.dinucleotide_encode()\n",
    "\n",
    "fw_fasta = dict[\"forward\"]\n",
    "rc_fasta = dict[\"reverse\"]\n",
    "readout = dict[\"readout\"]\n",
    "\n",
    "\n",
    "readout = np.log2(readout)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(readout.reshape(-1, 1))\n",
    "readout = scaler.transform(readout.reshape(-1, 1))\n",
    "readout = readout.flatten()\n",
    "readout = np.ndarray.tolist(readout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2440, 171, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dim_num = fw_fasta.shape\n",
    "dim_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['>A:HNF4A-CHMOD_CHR10:11917871-11917984_[CHR10:11917842-11918013]',\n",
       "       '>A:HNF4A-CHMOD_CHR10:34165653-34165745_[CHR10:34165613-34165784]',\n",
       "       '>A:HNF4A-CHMOD_CHR10:52009954-52010059_[CHR10:52009921-52010092]',\n",
       "       ...,\n",
       "       '>R:HNF4A-NOMOD_CHR9:89020204-89020327_[CHR9:89020180-89020351]',\n",
       "       '>R:HNF4A-NOMOD_CHR9:98508868-98508933_[CHR9:98508815-98508986]',\n",
       "       '>R:HNF4A-NOMOD_CHRY:18213828-18213963_[CHRY:18213810-18213981]'],\n",
       "      dtype='<U309')"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "def read_fasta_into_list():\n",
    "        all_seqs = []\n",
    "        with open('./sequences.fa', \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.upper()\n",
    "                if line[0] == '>':\n",
    "                    continue\n",
    "                else:\n",
    "                    all_seqs.append(line)\n",
    "        return all_seqs\n",
    "\n",
    "def read_fasta_name_into_array():\n",
    "        all_seqs = []\n",
    "        with open('./sequences.fa', \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.upper()\n",
    "                if line[0] == '>':\n",
    "                    all_seqs.append(line)\n",
    "                    #continue\n",
    "                else:\n",
    "                    #all_seqs.append(line)\n",
    "                    pass\n",
    "                \n",
    "        return np.array(all_seqs)\n",
    "\n",
    "read_fasta_name_into_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "forward (InputLayer)            [(None, 171, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reverse (InputLayer)            [(None, 171, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "convolution_layer (ConvolutionL (None, 160, 512)     25088       forward[0][0]                    \n",
      "                                                                 reverse[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 320, 512)     0           convolution_layer[0][0]          \n",
      "                                                                 convolution_layer[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 320, 512)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 512)       0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            513         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 25,601\n",
      "Trainable params: 25,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['convolution_layer/bias:0'] when minimizing the loss.\n",
      "38/38 [==============================] - 6s 109ms/step - loss: 0.0357 - coeff_determination: -0.3413 - spearman_fn: 0.2548\n",
      "Epoch 2/30\n",
      "38/38 [==============================] - 4s 102ms/step - loss: 0.0222 - coeff_determination: 0.4449 - spearman_fn: 0.6078\n",
      "Epoch 3/30\n",
      "38/38 [==============================] - 4s 112ms/step - loss: 0.0176 - coeff_determination: 0.4921 - spearman_fn: 0.7008\n",
      "Epoch 4/30\n",
      "22/38 [================>.............] - ETA: 1s - loss: 0.0153 - coeff_determination: 0.5147 - spearman_fn: 0.6856"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b2ba51e70dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m nn_model('./sequences.fa', \n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m'./wt_readout.dat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d2b1b2ec0f55>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fasta_file, readout_file, filters, kernel_size, pool_type, regularizer, activation_type, epochs, batch_size, loss_func, optimizer)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m#self.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;31m#self.cross_val_binning()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d2b1b2ec0f55>\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;31m#history = model.fit({'forward': fwd_train, 'reverse': rc_train}, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.0, callbacks = [callback])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfwd_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reverse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrc_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfwd_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reverse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrc_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfwd_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reverse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrc_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/museam_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/museam_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/museam_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/museam_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/museam_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/museam_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/museam_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "nn_model('./sequences.fa', \n",
    "        './wt_readout.dat', \n",
    "        filters=parameters_dict[\"filters\"], \n",
    "        kernel_size=parameters_dict[\"kernel_size\"], \n",
    "        pool_type=parameters_dict[\"pool_type\"], \n",
    "        regularizer=parameters_dict[\"regularizer\"],\n",
    "        activation_type=parameters_dict[\"activation_type\"], \n",
    "        epochs=parameters_dict[\"epochs\"], \n",
    "        batch_size=parameters_dict[\"batch_size\"], \n",
    "        loss_func=parameters_dict[\"loss_func\"], \n",
    "        optimizer=parameters_dict[\"optimizer\"])\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}